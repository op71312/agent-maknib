#!/usr/bin/env python3
"""
à¸ªà¸„à¸£à¸´à¸›à¸•à¹Œà¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡ LLM à¹ƒà¸«à¹‰à¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸”
à¸£à¸±à¸™à¸à¹ˆà¸­à¸™à¹€à¸£à¸´à¹ˆà¸¡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œà¹€à¸žà¸·à¹ˆà¸­ optimize performance
"""

import os
import psutil
import torch

def optimize_system_for_llm():
    """à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¸£à¸°à¸šà¸šà¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ LLM à¸—à¸³à¸‡à¸²à¸™à¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸”"""
    
    print("ðŸš€ à¸à¸³à¸¥à¸±à¸‡à¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¸£à¸°à¸šà¸šà¸ªà¸³à¸«à¸£à¸±à¸š LLM...")
    
    # 1. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² environment variables à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§
    os.environ['OMP_NUM_THREADS'] = str(psutil.cpu_count())
    os.environ['MKL_NUM_THREADS'] = str(psutil.cpu_count())
    os.environ['OPENBLAS_NUM_THREADS'] = str(psutil.cpu_count())
    os.environ['VECLIB_MAXIMUM_THREADS'] = str(psutil.cpu_count())
    os.environ['NUMEXPR_NUM_THREADS'] = str(psutil.cpu_count())
    
    # 2. à¸›à¸´à¸” Python garbage collection à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§ (à¹€à¸žà¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§)
    import gc
    gc.disable()
    
    # 3. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² CUDA à¸–à¹‰à¸²à¸¡à¸µ
    if torch.cuda.is_available():
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # à¹€à¸›à¸´à¸” async execution
        torch.backends.cudnn.benchmark = True    # à¹€à¸žà¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§ cudnn
        torch.backends.cudnn.deterministic = False
        print(f"âœ… CUDA optimized: {torch.cuda.get_device_name()}")
    
    # 4. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² memory management
    try:
        import ctypes
        libc = ctypes.CDLL("libc.so.6")
        # à¸›à¸´à¸” malloc mmap threshold à¹€à¸žà¸·à¹ˆà¸­à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž
        libc.mallopt(1, 0)  # M_MXFAST
        print("âœ… Memory optimization applied")
    except:
        pass
    
    print("âœ… à¸£à¸°à¸šà¸šà¸›à¸£à¸±à¸šà¹à¸•à¹ˆà¸‡à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢!")

def get_optimal_llama_config():
    """à¸„à¸·à¸™à¸„à¹ˆà¸² config à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸ªà¸³à¸«à¸£à¸±à¸šà¸£à¸°à¸šà¸šà¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™"""
    cpu_count = psutil.cpu_count()
    available_memory = psutil.virtual_memory().available // (1024**3)  # GB
    
    # à¸›à¸£à¸±à¸š config à¸•à¸²à¸¡ hardware
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
        print(f"ðŸ” GPU Memory: {gpu_memory}GB")
        
        if gpu_memory >= 8:  # High-end GPU
            return {
                'n_ctx': 2048,
                'n_threads': min(16, cpu_count),
                'n_batch': 1024,
                'n_gpu_layers': -1,  # à¹ƒà¸Šà¹‰ GPU à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
                'use_mlock': True,
                'use_mmap': True,
                'rope_freq_base': 10000.0,
                'flash_attn': True
            }
        else:  # Mid-range GPU
            return {
                'n_ctx': 1536,
                'n_threads': min(12, cpu_count),
                'n_batch': 512,
                'n_gpu_layers': 20,  # à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™à¹ƒà¸™ GPU
                'use_mlock': True,
                'use_mmap': True
            }
    else:  # CPU only
        return {
            'n_ctx': 1024,
            'n_threads': cpu_count,
            'n_batch': 256,
            'n_gpu_layers': 0,
            'use_mlock': False,
            'use_mmap': True
        }

if __name__ == "__main__":
    optimize_system_for_llm()
    config = get_optimal_llama_config()
    print("ðŸ“‹ Optimal config:", config)

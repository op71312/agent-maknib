{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4tRNYQeP9b4",
        "outputId": "a48ad7c3-ec3f-4cf3-b724-1880cb09f1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sEjR-PRpQAk9",
        "outputId": "ef7f821e-b2e4-4c0e-e092-3774676b5c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (1:14.0-55~exp2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Collecting faiss-gpu-cu11\n",
            "  Using cached faiss_gpu_cu11-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting llama-cpp-python\n",
            "  Using cached llama_cpp_python-0.3.13.tar.gz (50.1 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting attacut\n",
            "  Using cached attacut-1.0.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting langchain_community\n",
            "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.1.2-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting numpy<2 (from faiss-gpu-cu11)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu11) (24.2)\n",
            "Collecting nvidia-cuda-runtime-cu11>=11.8.89 (from faiss-gpu-cu11)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cublas-cu11>=11.11.3.6 (from faiss-gpu-cu11)\n",
            "  Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting docopt>=0.6.2 (from attacut)\n",
            "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire>=0.1.3 (from attacut)\n",
            "  Using cached fire-0.7.0.tar.gz (87 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nptyping>=0.2.0 (from attacut)\n",
            "  Using cached nptyping-2.5.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from attacut) (1.17.0)\n",
            "Collecting ssg>=0.0.4 (from attacut)\n",
            "  Using cached ssg-0.0.8-py3-none-any.whl.metadata (762 bytes)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.1.3->attacut) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Collecting python-crfsuite>=0.9.6 (from ssg>=0.0.4->attacut)\n",
            "  Using cached python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Using cached faiss_gpu_cu11-1.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.0 MB)\n",
            "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "Using cached attacut-1.0.6-py3-none-any.whl (1.3 MB)\n",
            "Using cached langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "Downloading pythainlp-5.1.2-py3-none-any.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Using cached nptyping-2.5.0-py3-none-any.whl (37 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Downloading nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ssg-0.0.8-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m732.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: llama-cpp-python, docopt, fire\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.13-cp311-cp311-linux_x86_64.whl size=4194620 sha256=7a0136a3665135bc524fa01011bb7a72f4ace3052ecd1dc77396dcbe3d9f0957\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6f/69/0760a999cdf90aacd874c03176ef0fd39a00642c82b4460397\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9c393ccfacb4e29f995a8b7761f9e29b1112539a7a289149fdeaf7939c9e96d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=ca25cbb705410ca9707a4a0a4e8957b9bf55b80e08a1e0f589ddc168602adce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llama-cpp-python docopt fire\n",
            "Installing collected packages: docopt, python-dotenv, python-crfsuite, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cublas-cu11, numpy, mypy-extensions, marshmallow, httpx-sse, fire, diskcache, typing-inspect, ssg, pythainlp, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nptyping, llama-cpp-python, faiss-gpu-cu11, pydantic-settings, nvidia-cusolver-cu12, dataclasses-json, sentence-transformers, attacut, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attacut-1.0.6 dataclasses-json-0.6.7 diskcache-5.6.3 docopt-0.6.2 faiss-gpu-cu11-1.11.0 fire-0.7.0 httpx-sse-0.4.1 langchain_community-0.3.27 llama-cpp-python-0.3.13 marshmallow-3.26.1 mypy-extensions-1.1.0 nptyping-2.5.0 numpy-1.26.4 nvidia-cublas-cu11-11.11.3.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.10.1 pythainlp-5.1.2 python-crfsuite-0.9.11 python-dotenv-1.1.1 sentence-transformers-5.0.0 ssg-0.0.8 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bd78336400e24ade8c7cc622a26c4c9f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "!apt-get update && apt-get install -y libomp-dev\n",
        "!pip install --upgrade faiss-gpu-cu11 sentence-transformers llama-cpp-python langchain attacut langchain_community pythainlp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZTK5eefS2Xl",
        "outputId": "e1b3ef8c-a1fd-4fa3-87ff-6c5812e707be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from typing import List, Optional, Dict, Any\n",
        "from pathlib import Path\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from pydantic import Field, BaseModel\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema.retriever import BaseRetriever\n",
        "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v3FKocZ_Xbou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de321ca-306f-42ca-a1ea-3a6c22af623b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constants configured\n",
            "Vector DB: /content/drive/MyDrive/AlphaZero_Backups/vector_database\n",
            "Model: /content/drive/MyDrive/AlphaZero_Backups/model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# Configuration for i5-12th gen + RTX 4050\n",
        "VECTOR_DB_DIR = \"/content/drive/MyDrive/AlphaZero_Backups/vector_database\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/AlphaZero_Backups/model/unsloth.Q4_K_M.gguf\"\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "\n",
        "print(\"Constants configured\")\n",
        "print(f\"Vector DB: {VECTOR_DB_DIR}\")\n",
        "print(f\"Model: {MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4fFdJL-fXahR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe0decf-1193-48e0-d7ab-aaa58c71bd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedEmbeddings class defined\n"
          ]
        }
      ],
      "source": [
        "class OptimizedEmbeddings(Embeddings):\n",
        "    \"\"\"Optimized embedding class for i5-12th gen CPU\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, device: str = \"cpu\"):\n",
        "        print(f\"Initializing OptimizedEmbeddings with device: {device}\")\n",
        "        self.device = device\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        if self.device == \"cpu\":\n",
        "            self.model.to(\"cpu\")\n",
        "            torch.set_num_threads(4)  # Optimal for i5-12th gen\n",
        "            print(\"- Configured for CPU inference with 4 threads\")\n",
        "        else:\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed documents with batch processing\"\"\"\n",
        "        batch_size = 32 if self.device == \"cpu\" else 64\n",
        "\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch, convert_to_numpy=True, show_progress_bar=False\n",
        "                )\n",
        "            embeddings.extend(batch_embeddings.tolist())\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed single query\"\"\"\n",
        "        return self.embed_documents([text])[0]\n",
        "\n",
        "print(\"OptimizedEmbeddings class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q6bo6jfqXdgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d2106ff-8be6-4b43-905f-996ce43617a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedReranker class defined\n"
          ]
        }
      ],
      "source": [
        "class OptimizedReranker:\n",
        "    \"\"\"Optimized reranker for RTX 4050 with quantization\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = RERANKER_MODEL_NAME, use_4bit: bool = True):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.max_length = 512\n",
        "\n",
        "        print(f\"Initializing OptimizedReranker on {self.device}\")\n",
        "\n",
        "        try:\n",
        "            if use_4bit and self.device == \"cuda\":\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16,\n",
        "                    bnb_4bit_use_double_quant=True\n",
        "                )\n",
        "                self.model = CrossEncoder(\n",
        "                    model_name, device=self.device, max_length=self.max_length,\n",
        "                    quantization_config=quantization_config\n",
        "                )\n",
        "                print(\"- Loaded with 4-bit quantization\")\n",
        "            else:\n",
        "                self.model = CrossEncoder(model_name, device=self.device, max_length=self.max_length)\n",
        "                print(\"- Loaded standard model\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}, falling back to CPU\")\n",
        "            self.device = \"cpu\"\n",
        "            self.model = CrossEncoder(model_name, device=self.device, max_length=self.max_length)\n",
        "\n",
        "    def rerank(self, query: str, docs: List[Document], batch_size: int = 32) -> List[Document]:\n",
        "        \"\"\"Rerank documents with optimized batching\"\"\"\n",
        "        if not docs:\n",
        "            return docs\n",
        "\n",
        "        pairs = [(query, doc.page_content[:self.max_length]) for doc in docs]\n",
        "\n",
        "        all_scores = []\n",
        "        for i in range(0, len(pairs), batch_size):\n",
        "            batch_pairs = pairs[i:i+batch_size]\n",
        "            with torch.no_grad():\n",
        "                batch_scores = self.model.predict(batch_pairs, show_progress_bar=False)\n",
        "            all_scores.extend(batch_scores.tolist())\n",
        "\n",
        "        # Sort by scores (descending)\n",
        "        scored_docs = list(zip(all_scores, docs))\n",
        "        scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        return [doc for _, doc in scored_docs]\n",
        "\n",
        "print(\"OptimizedReranker class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vvsqlWiHXfZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c7f6d7-9a17-48a0-b1a8-906789d78230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedThaiChunker class defined\n"
          ]
        }
      ],
      "source": [
        "class OptimizedThaiChunker:\n",
        "    \"\"\"Optimized text chunker for Thai\"\"\"\n",
        "\n",
        "    def __init__(self, max_chunk_size: int = 256, overlap: int = 32):\n",
        "        self.max_chunk_size = max_chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[Document]:\n",
        "        \"\"\"Split text into optimized chunks\"\"\"\n",
        "        sentences = sent_tokenize(text.strip(), engine=\"crfcut\")\n",
        "\n",
        "        if not sentences:\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            sentence_length = len(sentence)\n",
        "\n",
        "            if current_length + sentence_length > self.max_chunk_size and current_chunk:\n",
        "                chunks.append(Document(\n",
        "                    page_content=current_chunk.strip(),\n",
        "                    metadata={**metadata, \"chunk_size\": len(current_chunk)}\n",
        "                ))\n",
        "\n",
        "                overlap_text = self._get_overlap_text(current_chunk, self.overlap)\n",
        "                current_chunk = overlap_text + \" \" + sentence if overlap_text else sentence\n",
        "                current_length = len(current_chunk)\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    current_chunk += \" \" + sentence\n",
        "                else:\n",
        "                    current_chunk = sentence\n",
        "                current_length += sentence_length + (1 if current_chunk != sentence else 0)\n",
        "\n",
        "        if current_chunk.strip():\n",
        "            chunks.append(Document(\n",
        "                page_content=current_chunk.strip(),\n",
        "                metadata={**metadata, \"chunk_size\": len(current_chunk)}\n",
        "            ))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _get_overlap_text(self, text: str, overlap_size: int) -> str:\n",
        "        \"\"\"Get overlap text from the end of previous chunk\"\"\"\n",
        "        if len(text) <= overlap_size:\n",
        "            return text\n",
        "        return text[-overlap_size:]\n",
        "\n",
        "print(\"OptimizedThaiChunker class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BsSyu0zjlzYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c782606-b774-45d9-ba40-2a1695489d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedRetriever class defined\n"
          ]
        }
      ],
      "source": [
        "class OptimizedRetriever(BaseRetriever):\n",
        "    \"\"\"Optimized retriever with efficient reranking\"\"\"\n",
        "\n",
        "    vector_store: FAISS = Field(...)\n",
        "    reranker: OptimizedReranker = Field(default_factory=OptimizedReranker)\n",
        "    base_k: int = Field(default=10)\n",
        "    max_k: int = Field(default=20)\n",
        "    rerank_k: int = Field(default=5)\n",
        "    confidence_threshold: float = Field(default=0.7)\n",
        "\n",
        "    def __init__(self, vector_store: FAISS, **kwargs):\n",
        "        super().__init__(vector_store=vector_store, **kwargs)\n",
        "\n",
        "    def filter_by_metadata(self, docs: List[Document], filters: Dict[str, Any]) -> List[Document]:\n",
        "        \"\"\"Efficient metadata filtering\"\"\"\n",
        "        if not filters:\n",
        "            return docs\n",
        "\n",
        "        filtered_docs = []\n",
        "        for doc in docs:\n",
        "            match = True\n",
        "            for key, value in filters.items():\n",
        "                if key not in doc.metadata:\n",
        "                    match = False\n",
        "                    break\n",
        "\n",
        "                doc_value = doc.metadata[key]\n",
        "                if isinstance(value, list):\n",
        "                    if doc_value not in value:\n",
        "                        match = False\n",
        "                        break\n",
        "                elif isinstance(value, str):\n",
        "                    if value.lower() not in doc_value.lower():\n",
        "                        match = False\n",
        "                        break\n",
        "                else:\n",
        "                    if doc_value != value:\n",
        "                        match = False\n",
        "                        break\n",
        "\n",
        "            if match:\n",
        "                filtered_docs.append(doc)\n",
        "\n",
        "        return filtered_docs\n",
        "\n",
        "    def _get_relevant_documents(self, query: str, metadata_filters: Optional[Dict[str, Any]] = None,\n",
        "                               use_reranking: bool = True, *, run_manager=None) -> List[Document]:\n",
        "        \"\"\"Optimized document retrieval\"\"\"\n",
        "\n",
        "        # Vector similarity search\n",
        "        results = self.vector_store.similarity_search_with_score(query, k=self.base_k)\n",
        "\n",
        "        # Adaptive k based on confidence\n",
        "        if results and len(results) > 0:\n",
        "            top_score = results[0][1]\n",
        "            if top_score > self.confidence_threshold:\n",
        "                results = self.vector_store.similarity_search_with_score(query, k=self.max_k)\n",
        "\n",
        "        docs = [doc for doc, _ in results]\n",
        "\n",
        "        # Apply metadata filters\n",
        "        if metadata_filters:\n",
        "            docs = self.filter_by_metadata(docs, metadata_filters)\n",
        "\n",
        "        # Rerank only top results\n",
        "        if use_reranking and docs and len(docs) > 1:\n",
        "            docs_to_rerank = docs[:self.rerank_k]\n",
        "            remaining_docs = docs[self.rerank_k:]\n",
        "            reranked_docs = self.reranker.rerank(query, docs_to_rerank)\n",
        "            docs = reranked_docs + remaining_docs\n",
        "\n",
        "        return docs\n",
        "\n",
        "print(\"OptimizedRetriever class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kqF5I8wRXgwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aedebb1-ad22-4b6c-cf7d-83be2a980d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions and prompt template defined\n"
          ]
        }
      ],
      "source": [
        "def optimized_truncate_context(docs: List[Document], max_chars: int = 1500) -> List[Document]:\n",
        "    \"\"\"Optimized context truncation\"\"\"\n",
        "    if not docs:\n",
        "        return docs\n",
        "\n",
        "    result = []\n",
        "    current_chars = 0\n",
        "\n",
        "    for doc in docs:\n",
        "        content = doc.page_content\n",
        "        content_length = len(content)\n",
        "\n",
        "        if current_chars + content_length <= max_chars:\n",
        "            result.append(doc)\n",
        "            current_chars += content_length\n",
        "        else:\n",
        "            remaining_chars = max_chars - current_chars\n",
        "            if remaining_chars > 100:\n",
        "                partial_content = content[:remaining_chars].rsplit(' ', 1)[0]\n",
        "                partial_doc = Document(\n",
        "                    page_content=partial_content + \"...\",\n",
        "                    metadata={**doc.metadata, \"truncated\": True}\n",
        "                )\n",
        "                result.append(partial_doc)\n",
        "            break\n",
        "\n",
        "    return result\n",
        "\n",
        "# Prompt template\n",
        "structured_classification_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "คุณเป็นผู้เชี่ยวชาญหมากหนีบและกลยุทธ์สามก๊ก โปรดวิเคราะห์การเดินหมากต่อไปนี้:\n",
        "\n",
        "กติกาหมากหนีบ:\n",
        "- กระดาน 8×8 ผู้เล่นฝ่ายละ 8 ตัว\n",
        "- เดินตรง (ขึ้น/ลง/ซ้าย/ขวา) ห้ามเดินเฉียงหรือข้ามหมาก\n",
        "- การหนีบ 2 แบบ:\n",
        "  1. หนีบด้วยตัวเดียว: เรา 1 ตัว ล้อมศัตรูในแนวเดียวกัน\n",
        "  2. หนีบด้วยสองตัว: เรา 2 ตัว ศัตรูอยู่ตรงกลาง\n",
        "- ชนะเมื่อ: กินหมากศัตรูหมด, ขังศัตรู, หรือมีหมากมากกว่า\n",
        "การเดินหมาก:\n",
        "{question}\n",
        "\n",
        "ข้อมูลกลยุทธ์:\n",
        "{context}\n",
        "\n",
        "ตอบ:\n",
        "กลยุทธ์: [ชื่อกลยุทธ์]\n",
        "เหตุผล: [อธิบายสั้นๆว่าเป็นเดินหมากยังไงถึงตรงกับกลยุทธ์นี้]\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "print(\"Helper functions and prompt template defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n19g4YCdXiLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9deb4b26-a0c0-4bf6-af38-7cf97262e790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up Optimized RAG System...\n",
            "Initializing OptimizedEmbeddings with device: cpu\n",
            "- Configured for CPU inference with 4 threads\n",
            "Initializing OptimizedReranker on cuda\n",
            "Error: No package metadata was found for bitsandbytes, falling back to CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System ready! Vector store contains 56 documents\n"
          ]
        }
      ],
      "source": [
        "def setup_optimized_system():\n",
        "    \"\"\"Setup optimized RAG system\"\"\"\n",
        "    print(\"Setting up Optimized RAG System...\")\n",
        "\n",
        "    # Initialize embeddings (CPU)\n",
        "    embeddings = OptimizedEmbeddings(device=\"cpu\")\n",
        "\n",
        "    # Load vector store\n",
        "    vector_store = FAISS.load_local(\n",
        "        VECTOR_DB_DIR,\n",
        "        embeddings,\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "\n",
        "    # Initialize retriever\n",
        "    retriever = OptimizedRetriever(vector_store=vector_store)\n",
        "\n",
        "    # Initialize LLM\n",
        "    llm = LlamaCpp(\n",
        "        model_path=MODEL_PATH,\n",
        "        temperature=0.1,\n",
        "        max_tokens=512,\n",
        "        n_ctx=2048,\n",
        "        n_gpu_layers=35,\n",
        "        n_threads=4,\n",
        "        verbose=False,\n",
        "        stop=[\"\\n\\n\"],\n",
        "        use_mmap=True,\n",
        "        use_mlock=True\n",
        "    )\n",
        "\n",
        "    total_docs = len(vector_store.docstore._dict)\n",
        "    print(f\"System ready! Vector store contains {total_docs} documents\")\n",
        "\n",
        "    return retriever, llm\n",
        "\n",
        "# Initialize system\n",
        "retriever, llm = setup_optimized_system()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nrkPY61cXkVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d600ad4d-a67b-495b-c2b2-d86dafdb0d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification function defined\n"
          ]
        }
      ],
      "source": [
        "def optimized_classify_moves(moves_desc: str, retriever: OptimizedRetriever, llm: LlamaCpp,\n",
        "                           max_context_chars: int = 1500, strategy_hint: Optional[str] = None,\n",
        "                           use_reranking: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"Optimized classification function\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Prepare metadata filters\n",
        "    metadata_filters = None\n",
        "    if strategy_hint:\n",
        "        metadata_filters = {\"ชื่อกลยุทธ์\": strategy_hint}\n",
        "\n",
        "    # Retrieve relevant documents\n",
        "    docs = retriever._get_relevant_documents(\n",
        "        moves_desc,\n",
        "        metadata_filters=metadata_filters,\n",
        "        use_reranking=use_reranking\n",
        "    )\n",
        "\n",
        "    # Truncate context\n",
        "    docs = optimized_truncate_context(docs, max_chars=max_context_chars)\n",
        "\n",
        "    # Prepare context\n",
        "    context_parts = []\n",
        "    for doc in docs:\n",
        "        strategy_name = doc.metadata.get('ชื่อกลยุทธ์', 'ไม่ระบุ')\n",
        "        category = doc.metadata.get('หมวด', 'ไม่ระบุ')\n",
        "        context_parts.append(f\"[{strategy_name}|{category}] {doc.page_content}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Generate response\n",
        "    prompt = structured_classification_prompt.format(context=context, question=moves_desc)\n",
        "    result = llm.invoke(prompt)\n",
        "\n",
        "    # Calculate metrics\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    response = {\n",
        "        \"classification\": result.strip(),\n",
        "        \"context_chars\": len(context),\n",
        "        \"documents_used\": len(docs),\n",
        "        \"processing_time\": duration,\n",
        "        \"reranking_used\": use_reranking\n",
        "    }\n",
        "\n",
        "    print(f\"Classification completed in {duration:.2f}s using {len(docs)} documents\")\n",
        "\n",
        "    return response\n",
        "\n",
        "print(\"Classification function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVal-KlHXkqM",
        "outputId": "9330d501-2fcd-416e-ada1-c53282892e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TESTING CLASSIFICATION ===\n",
            "Classification completed in 176.12s using 6 documents\n",
            "\n",
            "=== RESULT ===\n",
            "ผลลัพธ์: [ระบุผลลัพธ์ที่คาดหวังจากกลยุทธ์นี้]\n",
            "เมื่อวันที่ 15 กุมภาพันธ์ 2566 เวลา 10.00 น. ณ ห้องประชุมชั้น 3 อาคารรัฐสภาแห่งใหม่\n",
            "คณะกรรมาธิการวิสามัญพิจารณาร่างพระราชบัญญัติประกอบรัฐธรรมนูญ (ฉบับที่…) พ.ศ. … (แก้ไขเพิ่มเติมให้สอดคล้องกับบทบัญญัติรัฐธรรมนูญแห่งราชอาณาจักรไทย พุทธศักราช 2560) มี พลเอก สุรเชษฐ์ ชัยวงศ์ เป็นประธาน\n",
            "ในการประชุมครั้งนี้ ที่ประชุมได้พิจารณาร่างพระราชบัญญัติประกอบรัฐธรรมนูญ (ฉบับที่…) พ.ศ. … (แก้ไขเพิ่มเติมให้สอดคล้องกับบทบัญญัติรัฐธรรมนูญแห่งราชอาณาจักรไทย พุทธศักราช 2560) และมีมติเห็นชอบกับร่างพระราชบัญญัติดังกล่าว\n",
            "จากนั้น ที่ประชุมได้พิจารณาร่างพระราชบัญญัติประกอบรัฐธรรมนูญ (ฉบับที่…) พ.ศ. … (แก้ไขเพิ่มเติมให้สอดคล้องกับบทบัญญัติรัฐธรรมนูญแห่งราชอาณาจักรไทย พุทธศักราช 2560) และมีมติเห็นชอบกับร่างพระราชบัญญัติดังกล่าว\n",
            "จากนั้น ที่ประชุมได้พิจารณาร่างพระราชบัญญัติประกอบรัฐธรรมนูญ (ฉบับที่…) พ.ศ. … (แก้ไขเพิ่มเติมให้สอดคล้องกับบทบัญญัติรัฐธรรมนูญแห่งราชอาณาจักรไทย พุทธศักราช 2560) และมีมติเห็นชอบกับร่างพระราชบัญญัติดังกล่าว\n",
            "จากนั้น ที่ประชุมได้พิจารณาร่างพระราชบัญญัติประกอบรัฐธรรมนูญ (ฉบับที่…) พ.ศ. … (แก้ไขเพิ่มเติมให้สอดคล้องกับบทบัญญัติรัฐธรรมนูญแห่งราชอาณาจักรไทย พุทธ\n",
            "\n",
            "Metrics:\n",
            "- Processing time: 176.12s\n",
            "- Documents used: 6\n",
            "- Context chars: 1635\n",
            "- Reranking used: True\n"
          ]
        }
      ],
      "source": [
        "# Example moves for testing\n",
        "example_moves = \"\"\"\n",
        "ตาที่ 1 : ผู้เล่น 1 เคลื่อนที่หมากจากแถว 0 หลัก 7 ไปแถว 1 หลัก 7\n",
        "ตาที่ 3 : ผู้เล่น 1 เคลื่อนที่หมากจากแถว 6 หลัก 4 ไปแถว 7 หลัก 4\n",
        "ตาที่ 5 : ผู้เล่น 1 เคลื่อนที่หมากจากแถว 0 หลัก 1 ไปแถว 0 หลัก 0\n",
        "\"\"\"\n",
        "\n",
        "print(\"=== TESTING CLASSIFICATION ===\")\n",
        "result = optimized_classify_moves(example_moves, retriever, llm)\n",
        "\n",
        "print(\"\\n=== RESULT ===\")\n",
        "print(result[\"classification\"])\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"- Processing time: {result['processing_time']:.2f}s\")\n",
        "print(f\"- Documents used: {result['documents_used']}\")\n",
        "print(f\"- Context chars: {result['context_chars']}\")\n",
        "print(f\"- Reranking used: {result['reranking_used']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QlNfyROiezPa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
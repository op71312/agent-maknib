{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auez5P1oUMdD",
        "outputId": "5f15601b-8ce4-41bf-b823-a69993b338fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting ray\n",
            "  Downloading ray-2.47.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray) (8.2.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray) (4.24.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray) (0.25.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.47.1-cp311-cp311-manylinux2014_x86_64.whl (68.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.9/68.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ray\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ray-2.47.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium numpy torch ray"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "import shutil\n",
        "import gc\n",
        "from typing import List, Dict, Tuple"
      ],
      "metadata": {
        "id": "NC7Jmh0VUOFc"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ========== PYTORCH SETUP ==========\n",
        "# à¹€à¸à¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§\n",
        "torch.backends.cudnn.benchmark = True\n",
        "# à¸ªà¸³à¸«à¸£à¸±à¸š GPU à¹ƒà¸«à¸¡à¹ˆ\n",
        "torch.set_float32_matmul_precision('high')\n"
      ],
      "metadata": {
        "id": "0t3R0zPedc9q"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== ENHANCED METRICS & LOGGING ==========\n",
        "class MetricsLogger:\n",
        "    def __init__(self):\n",
        "        self.iteration_metrics = []\n",
        "\n",
        "    def log_iteration_start(self, iteration: int):\n",
        "        self.current_iter = {\n",
        "            'iteration': iteration,\n",
        "            'start_time': time.perf_counter(),\n",
        "            'self_play_time': 0,\n",
        "            'training_time': 0,\n",
        "            'evaluation_time': 0\n",
        "        }\n",
        "\n",
        "    def log_self_play_metrics(self, stats: List[Dict], examples_count: int, self_play_time: float):\n",
        "        self.current_iter['self_play_time'] = self_play_time\n",
        "        self.current_iter['examples_count'] = examples_count\n",
        "        self.current_iter['game_stats'] = stats\n",
        "\n",
        "    def log_training_metrics(self, epoch_metrics: List[Dict], training_time: float):\n",
        "        self.current_iter['training_time'] = training_time\n",
        "        self.current_iter['epoch_metrics'] = epoch_metrics\n",
        "\n",
        "    def log_evaluation_metrics(self, eval_results: Dict, eval_time: float):\n",
        "        self.current_iter['evaluation_time'] = eval_time\n",
        "        self.current_iter['eval_results'] = eval_results\n",
        "\n",
        "    def finish_iteration(self):\n",
        "        self.current_iter['total_time'] = time.perf_counter() - self.current_iter['start_time']\n",
        "        self.iteration_metrics.append(self.current_iter.copy())\n",
        "        return self.current_iter"
      ],
      "metadata": {
        "id": "YuN8F01_kPf2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== NEURAL NETWORK ==========\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "class MakNeebNet(nn.Module):\n",
        "    def __init__(self, board_size=8, num_res_blocks=4, num_channels=32, action_size=4096):\n",
        "        super(MakNeebNet, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(1, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn_in = nn.BatchNorm2d(num_channels)\n",
        "        self.res_blocks = nn.ModuleList([ResNetBlock(num_channels) for _ in range(num_res_blocks)])\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
        "        self.policy_bn = nn.BatchNorm2d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_size * board_size, action_size)\n",
        "\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
        "        # BatchNorm is generally more stable than LayerNorm for this type of input\n",
        "        self.value_bn = nn.BatchNorm2d(1)\n",
        "        self.value_fc1 = nn.Linear(1 * board_size * board_size, 128)\n",
        "        self.value_fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1) # Add channel dimension\n",
        "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
        "\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Policy head\n",
        "        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        policy = policy.view(policy.size(0), -1)\n",
        "        policy = self.policy_fc(policy)\n",
        "\n",
        "        # Value head\n",
        "        value = F.relu(self.value_bn(self.value_conv(x)))\n",
        "        value = value.view(value.size(0), -1)\n",
        "        value = F.relu(self.value_fc1(value))\n",
        "        value = torch.tanh(self.value_fc2(value))\n",
        "\n",
        "        return F.log_softmax(policy, dim=1), value\n"
      ],
      "metadata": {
        "id": "cKfUYbxbUP9w"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== ENVIRONMENT ==========\n",
        "class MakNeebRLEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(MakNeebRLEnv, self).__init__()\n",
        "        self.board_size = 8\n",
        "        self.action_space = spaces.Discrete(self.board_size * self.board_size * self.board_size * self.board_size)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.board_size, self.board_size), dtype=np.int8)\n",
        "        self.reset()\n",
        "\n",
        "    def _encode_action(self, from_row, from_col, to_row, to_col):\n",
        "        return from_row * (8*8*8) + from_col * (8*8) + to_row * 8 + to_col\n",
        "\n",
        "    def _decode_action(self, action):\n",
        "        from_row = action // (8*8*8)\n",
        "        action %= (8*8*8)\n",
        "        from_col = action // (8*8)\n",
        "        action %= (8*8)\n",
        "        to_row = action // 8\n",
        "        to_col = action % 8\n",
        "        return (from_row, from_col), (to_row, to_col)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int8)\n",
        "        self.board[0, :] = 1\n",
        "        self.board[7, :] = -1\n",
        "        self.current_player = 1\n",
        "        self.turns_without_capture = 0\n",
        "        self.max_turns_without_capture = 50\n",
        "        return self.board.copy(), {\"current_player\": self.current_player}\n",
        "\n",
        "    def get_legal_actions(self):\n",
        "        legal_actions = []\n",
        "        for r_from in range(self.board_size):\n",
        "            for c_from in range(self.board_size):\n",
        "                if self.board[r_from, c_from] == self.current_player:\n",
        "                    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                        for i in range(1, self.board_size):\n",
        "                            r_to, c_to = r_from + dr * i, c_from + dc * i\n",
        "                            if not (0 <= r_to < self.board_size and 0 <= c_to < self.board_size):\n",
        "                                break\n",
        "                            if self.board[r_to, c_to] != 0:\n",
        "                                break\n",
        "                            action = self._encode_action(r_from, c_from, r_to, c_to)\n",
        "                            legal_actions.append(action)\n",
        "        return legal_actions\n",
        "\n",
        "    def step(self, action):\n",
        "        legal_actions = self.get_legal_actions()\n",
        "        if action not in legal_actions:\n",
        "            # Assign a large penalty for an illegal move and end the game\n",
        "            return self.board.copy(), -10.0, True, False, {\"error\": \"Illegal move\"}\n",
        "\n",
        "        (from_row, from_col), (to_row, to_col) = self._decode_action(action)\n",
        "        self.board[to_row, to_col] = self.current_player\n",
        "        self.board[from_row, from_col] = 0\n",
        "\n",
        "        captured_count = self._check_and_capture(to_row, to_col)\n",
        "        reward = captured_count * 2.0\n",
        "\n",
        "        if captured_count > 0:\n",
        "            self.turns_without_capture = 0\n",
        "        else:\n",
        "            self.turns_without_capture += 1\n",
        "\n",
        "        done, winner = self.get_game_status()\n",
        "        if done:\n",
        "            if winner == self.current_player:\n",
        "                reward += 20.0\n",
        "            elif winner == -self.current_player:\n",
        "                reward -= 20.0\n",
        "            elif winner == 0: # Draw\n",
        "                reward += 5.0\n",
        "\n",
        "        self.current_player *= -1\n",
        "        info = {\"current_player\": self.current_player, \"captured\": captured_count > 0}\n",
        "        return self.board.copy(), float(reward), done, False, info\n",
        "\n",
        "    def _check_and_capture(self, r, c):\n",
        "        total_captured = 0\n",
        "        opponent = -self.current_player\n",
        "\n",
        "        # Check both horizontal and vertical lines passing through (r, c)\n",
        "        for dr, dc in [(1, 0), (0, 1)]:\n",
        "            line_pieces = []\n",
        "            if dr == 1: # Vertical line\n",
        "                for i in range(self.board_size):\n",
        "                    if self.board[i, c] != 0:\n",
        "                        line_pieces.append({'player': self.board[i, c], 'pos': (i, c)})\n",
        "            else: # Horizontal line\n",
        "                for i in range(self.board_size):\n",
        "                    if self.board[r, i] != 0:\n",
        "                        line_pieces.append({'player': self.board[r, i], 'pos': (r, i)})\n",
        "\n",
        "            if len(line_pieces) < 2:\n",
        "                continue\n",
        "\n",
        "            captured_in_this_line = set()\n",
        "            my_indices = [i for i, p in enumerate(line_pieces) if p['player'] == self.current_player]\n",
        "\n",
        "            if len(my_indices) >= 2:\n",
        "                for i in range(len(my_indices) - 1):\n",
        "                    start_idx, end_idx = my_indices[i], my_indices[i+1]\n",
        "                    if end_idx > start_idx + 1:\n",
        "                        # Check if all pieces between are opponent's pieces\n",
        "                        is_all_opponent = all(line_pieces[k]['player'] == opponent\n",
        "                                              for k in range(start_idx + 1, end_idx))\n",
        "                        if is_all_opponent:\n",
        "                            for k in range(start_idx + 1, end_idx):\n",
        "                                captured_in_this_line.add(line_pieces[k]['pos'])\n",
        "\n",
        "            # Apply captures\n",
        "            for pos_r, pos_c in captured_in_this_line:\n",
        "                if self.board[pos_r, pos_c] == opponent: # Ensure we don't accidentally remove our own piece\n",
        "                    self.board[pos_r, pos_c] = 0\n",
        "                    total_captured += 1\n",
        "\n",
        "        return total_captured\n",
        "\n",
        "\n",
        "    def get_game_status(self):\n",
        "        player1_pieces = np.sum(self.board == 1)\n",
        "        player_minus_1_pieces = np.sum(self.board == -1)\n",
        "\n",
        "        if player1_pieces == 0:\n",
        "            return True, -1\n",
        "        if player_minus_1_pieces == 0:\n",
        "            return True, 1\n",
        "        if not self.get_legal_actions_for_player(-self.current_player):\n",
        "            # The next player has no legal moves, so the current player wins.\n",
        "            return True, self.current_player\n",
        "\n",
        "        if self.turns_without_capture >= self.max_turns_without_capture:\n",
        "            if player1_pieces > player_minus_1_pieces:\n",
        "                return True, 1\n",
        "            elif player_minus_1_pieces > player1_pieces:\n",
        "                return True, -1\n",
        "            else: # Draw\n",
        "                return True, 0\n",
        "\n",
        "        return False, 0 # Game not over\n",
        "\n",
        "    def get_legal_actions_for_player(self, player):\n",
        "        original_player = self.current_player\n",
        "        self.current_player = player\n",
        "        legal_actions = self.get_legal_actions()\n",
        "        self.current_player = original_player\n",
        "        return legal_actions\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = MakNeebRLEnv()\n",
        "        new_env.board = self.board.copy()\n",
        "        new_env.current_player = self.current_player\n",
        "        new_env.turns_without_capture = self.turns_without_capture\n",
        "        return new_env\n"
      ],
      "metadata": {
        "id": "UmsmeQ8LUS73"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MCTS ==========\n",
        "class MCTSNode:\n",
        "    def __init__(self, parent=None, prior_p=1.0):\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0\n",
        "        self.prior_p = prior_p\n",
        "\n",
        "    def expand(self, action_priors: List[Tuple[int, float]]):\n",
        "        for action, prior in action_priors:\n",
        "            if action not in self.children:\n",
        "                self.children[action] = MCTSNode(parent=self, prior_p=prior)\n",
        "\n",
        "    def select(self, c_puct: float) -> Tuple[int, 'MCTSNode']:\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            score = self._get_ucb_score(child, c_puct)\n",
        "            if score > best_score:\n",
        "                best_score, best_action, best_child = score, action, child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def _get_ucb_score(self, child: 'MCTSNode', c_puct: float) -> float:\n",
        "        # Q-value is from the child's perspective\n",
        "        q_value = -child.value()\n",
        "\n",
        "        # U-value encourages exploration\n",
        "        u_value = c_puct * child.prior_p * math.sqrt(self.visit_count) / (1 + child.visit_count)\n",
        "\n",
        "        return q_value + u_value\n",
        "\n",
        "    def value(self) -> float:\n",
        "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0\n",
        "\n",
        "    def backpropagate(self, value: float):\n",
        "        self.visit_count += 1\n",
        "        # The value is always from the perspective of the current player at this node\n",
        "        self.value_sum += value\n",
        "        if self.parent:\n",
        "            # The value must be inverted for the parent, as it's the other player's turn\n",
        "            self.parent.backpropagate(-value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model: nn.Module, device: torch.device, c_puct: float = 1.5, num_simulations: int = 50):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.c_puct = c_puct\n",
        "        self.num_simulations = num_simulations\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, env: MakNeebRLEnv) -> np.ndarray:\n",
        "        root = MCTSNode()\n",
        "\n",
        "        # At the start of a search, the root represents the state for the current player\n",
        "        initial_board_tensor = torch.tensor(\n",
        "            env.board * env.current_player, dtype=torch.float32\n",
        "        ).unsqueeze(0).to(self.device)\n",
        "\n",
        "        log_policy, value_tensor = self.model(initial_board_tensor)\n",
        "        policy = torch.exp(log_policy).squeeze(0).cpu().numpy()\n",
        "        value = value_tensor.item()\n",
        "\n",
        "        legal_actions = env.get_legal_actions()\n",
        "        if not legal_actions:\n",
        "            return np.zeros(env.action_space.n)\n",
        "\n",
        "        action_priors = [(action, policy[action]) for action in legal_actions]\n",
        "        root.expand(action_priors)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            sim_env = env.copy()\n",
        "\n",
        "            # --- Selection ---\n",
        "            while node.children:\n",
        "                action, node = node.select(self.c_puct)\n",
        "                sim_env.step(action)\n",
        "\n",
        "            # --- Expansion & Evaluation ---\n",
        "            done, winner = sim_env.get_game_status()\n",
        "            value = 0.0\n",
        "\n",
        "            if not done:\n",
        "                board_tensor = torch.tensor(\n",
        "                    sim_env.board * sim_env.current_player, dtype=torch.float32\n",
        "                ).unsqueeze(0).to(self.device)\n",
        "\n",
        "                log_policy, value_tensor = self.model(board_tensor)\n",
        "                policy = torch.exp(log_policy).squeeze(0).cpu().numpy()\n",
        "                value = value_tensor.item()\n",
        "\n",
        "                legal_actions_sim = sim_env.get_legal_actions()\n",
        "                if legal_actions_sim:\n",
        "                    action_priors_sim = [(action, policy[action]) for action in legal_actions_sim]\n",
        "                    node.expand(action_priors_sim)\n",
        "            else:\n",
        "                # Terminal node, value is determined by game result\n",
        "                if winner == sim_env.current_player:\n",
        "                    value = 1.0\n",
        "                elif winner == -sim_env.current_player:\n",
        "                    value = -1.0\n",
        "                # if winner is 0 (draw), value remains 0.0\n",
        "\n",
        "            # --- Backpropagation ---\n",
        "            # Value is from the perspective of the player at the expanded node.\n",
        "            # backpropagate handles negating it for parents.\n",
        "            node.backpropagate(value)\n",
        "\n",
        "        # Return visit count distribution as the policy\n",
        "        visit_counts = np.zeros(env.action_space.n)\n",
        "        for action, child in root.children.items():\n",
        "            visit_counts[action] = child.visit_count\n",
        "\n",
        "        if np.sum(visit_counts) == 0:\n",
        "            return np.ones(env.action_space.n) / env.action_space.n\n",
        "\n",
        "        action_probs = visit_counts / np.sum(visit_counts)\n",
        "        return action_probs\n"
      ],
      "metadata": {
        "id": "qIA0HIF7UV59"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== PARALLEL SELF-PLAY WORKER ==========\n",
        "\n",
        "def play_game(model_state_dict: dict, mcts_sims: int, c_puct: float, temperature: float, noise_alpha: float) -> Tuple[List[Tuple], Dict]:\n",
        "    \"\"\"\n",
        "    Plays a single game of self-play on the CPU.\n",
        "    This function is executed by each worker process.\n",
        "    \"\"\"\n",
        "    # 1. à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸° MCTS à¸ à¸²à¸¢à¹ƒà¸™ Worker à¹à¸•à¹ˆà¸¥à¸°à¸•à¸±à¸§ (à¸—à¸³à¸‡à¸²à¸™à¸šà¸™ CPU)\n",
        "    device = torch.device('cpu')\n",
        "    model = MakNeebNet().to(device)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    model.eval()\n",
        "    mcts = MCTS(model, device, c_puct, mcts_sims)\n",
        "    env = MakNeebRLEnv()\n",
        "\n",
        "    game_history = []\n",
        "    policy_entropies = []\n",
        "\n",
        "    # 2. à¸§à¸™à¸¥à¸¹à¸›à¹€à¸¥à¹ˆà¸™à¹€à¸à¸¡à¸ˆà¸™à¸ˆà¸š\n",
        "    while True:\n",
        "        board_state = env.board * env.current_player\n",
        "        action_probs = mcts.search(env)\n",
        "\n",
        "        # à¹€à¸à¸´à¹ˆà¸¡ Dirichlet noise à¹€à¸à¸·à¹ˆà¸­à¸à¸²à¸£à¸ªà¸³à¸£à¸§à¸ˆ\n",
        "        legal_actions = env.get_legal_actions()\n",
        "        if legal_actions:\n",
        "            noise = np.random.dirichlet([noise_alpha] * len(legal_actions))\n",
        "            for i, action in enumerate(legal_actions):\n",
        "                action_probs[action] = 0.75 * action_probs[action] + 0.25 * noise[i]\n",
        "            action_probs /= np.sum(action_probs)\n",
        "\n",
        "        # à¹ƒà¸Šà¹‰ Temperature à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸šà¸„à¸¸à¸¡à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸à¸•à¸²à¹€à¸”à¸´à¸™\n",
        "        temp = temperature if len(game_history) < 15 else 0\n",
        "        if temp > 0:\n",
        "            temp_probs = action_probs ** (1.0 / temp)\n",
        "            action = np.random.choice(len(temp_probs), p=temp_probs/np.sum(temp_probs))\n",
        "        else:\n",
        "            action = np.argmax(action_probs)\n",
        "\n",
        "        valid_probs = action_probs[action_probs > 1e-8]\n",
        "        policy_entropies.append(-np.sum(valid_probs * np.log(valid_probs)))\n",
        "\n",
        "        game_history.append((board_state, action_probs, env.current_player))\n",
        "\n",
        "        _, _, done, _, _ = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # 3. à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸£à¸à¸¶à¸à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¹€à¸à¸¡à¸ˆà¸š\n",
        "    _, winner = env.get_game_status()\n",
        "    training_examples = []\n",
        "    for board, probs, player in game_history:\n",
        "        reward = 0.0 if winner == 0 else (1.0 if winner == player else -1.0)\n",
        "        training_examples.append((board, probs, reward))\n",
        "\n",
        "    game_stats = {\n",
        "        'winner': winner,\n",
        "        'game_length': len(game_history),\n",
        "        'avg_policy_entropy': np.mean(policy_entropies) if policy_entropies else 0.0,\n",
        "    }\n",
        "\n",
        "    return training_examples, game_stats\n",
        "\n",
        "class SelfPlayWorker(mp.Process):\n",
        "    \"\"\"\n",
        "    Worker process à¸—à¸µà¹ˆà¸£à¸±à¸šà¸œà¸´à¸”à¸Šà¸­à¸šà¸à¸²à¸£à¹€à¸¥à¹ˆà¸™à¹€à¸à¸¡ (à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰ play_game)\n",
        "    \"\"\"\n",
        "    def __init__(self, job_queue, data_queue, model_state_dict, mcts_params):\n",
        "        super().__init__()\n",
        "        self.job_queue = job_queue\n",
        "        self.data_queue = data_queue\n",
        "        self.model_state_dict = model_state_dict\n",
        "        self.mcts_params = mcts_params\n",
        "\n",
        "    def run(self):\n",
        "        while True:\n",
        "            game_idx = self.job_queue.get()\n",
        "            if game_idx is None:  # à¸ªà¸±à¸à¸à¸²à¸“à¹ƒà¸«à¹‰à¸«à¸¢à¸¸à¸”à¸—à¸³à¸‡à¸²à¸™\n",
        "                break\n",
        "\n",
        "            examples, stats = play_game(self.model_state_dict, **self.mcts_params)\n",
        "            self.data_queue.put((examples, stats))\n",
        "\n",
        "def parallel_self_play(model, num_games, num_workers, mcts_params):\n",
        "    \"\"\"\n",
        "    à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸±à¸”à¸à¸²à¸£ Self-Play à¹à¸šà¸š Parallel\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # à¸ªà¹ˆà¸‡ state_dict à¹à¸—à¸™ model object à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢à¹ƒà¸™ multiprocessing\n",
        "    model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    job_queue = mp.Queue()\n",
        "    data_queue = mp.Queue()\n",
        "\n",
        "    for i in range(num_games):\n",
        "        job_queue.put(i)\n",
        "    for _ in range(num_workers):\n",
        "        job_queue.put(None) # à¸ªà¸±à¸à¸à¸²à¸“à¹ƒà¸«à¹‰ Worker à¸«à¸¢à¸¸à¸”\n",
        "\n",
        "    workers = [SelfPlayWorker(job_queue, data_queue, model_state_dict, mcts_params) for _ in range(num_workers)]\n",
        "\n",
        "    print(f\"ğŸš€ Launching {num_workers} parallel workers for {num_games} games...\")\n",
        "    for w in workers:\n",
        "        w.start()\n",
        "\n",
        "    all_examples, all_stats = [], []\n",
        "    for _ in range(num_games):\n",
        "        examples, stats = data_queue.get()\n",
        "        all_examples.extend(examples)\n",
        "        all_stats.append(stats)\n",
        "        if (len(all_stats)) % 10 == 0:\n",
        "             print(f\"  ...collected results from {len(all_stats)}/{num_games} games\")\n",
        "\n",
        "\n",
        "    for w in workers:\n",
        "        w.join()\n",
        "\n",
        "    return all_examples, all_stats"
      ],
      "metadata": {
        "id": "qO1bGdrOdyEi"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== TRAINING & EVALUATION ==========\n",
        "def enhanced_train_model(model, replay_buffer, optimizer, device, batch_size, num_epochs, entropy_weight):\n",
        "    model.train()\n",
        "    buffer_list = list(replay_buffer)\n",
        "    epoch_metrics_list = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        random.shuffle(buffer_list)\n",
        "        epoch_losses = {'total': 0, 'policy': 0, 'value': 0}\n",
        "        num_batches = 0\n",
        "\n",
        "        for i in range(0, len(buffer_list), batch_size):\n",
        "            batch = buffer_list[i:i + batch_size]\n",
        "            if not batch: continue\n",
        "            boards, target_policies, target_values = zip(*batch)\n",
        "            boards = torch.tensor(np.array(boards), dtype=torch.float32).to(device)\n",
        "            target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)\n",
        "            target_values = torch.tensor(target_values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            log_policies, predicted_values = model(boards)\n",
        "            policy_loss = -(target_policies * log_policies).sum(dim=1).mean()\n",
        "            value_loss = F.mse_loss(predicted_values, target_values)\n",
        "            policy_probs_clamped = torch.clamp(torch.exp(log_policies), 1e-8, 1.0)\n",
        "            entropy = -(policy_probs_clamped * torch.log(policy_probs_clamped)).sum(dim=1).mean()\n",
        "            total_loss = policy_loss + value_loss - entropy * entropy_weight\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_losses['total'] += total_loss.item()\n",
        "            epoch_losses['policy'] += policy_loss.item()\n",
        "            epoch_losses['value'] += value_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        if num_batches > 0:\n",
        "            for key in epoch_losses: epoch_losses[key] /= num_batches\n",
        "            epoch_metrics_list.append(epoch_losses)\n",
        "            print(f\"  Epoch {epoch + 1}/{num_epochs} | Avg Loss: {epoch_losses['total']:.4f} [P: {epoch_losses['policy']:.4f}, V: {epoch_losses['value']:.4f}]\")\n",
        "\n",
        "    overall_metrics = {k: np.mean([e[k] for e in epoch_metrics_list]) for k in epoch_metrics_list[0]} if epoch_metrics_list else {}\n",
        "    return overall_metrics\n",
        "\n",
        "def evaluate_against_baseline(model, device, baseline_agent_func, num_games=20):\n",
        "    model.eval()\n",
        "    mcts = MCTS(model, device, c_puct=1.0, num_simulations=40)\n",
        "    wins, draws, losses = 0, 0, 0\n",
        "    for _ in range(num_games):\n",
        "        env = MakNeebRLEnv()\n",
        "        model_is_player1 = random.choice([True, False])\n",
        "        while True:\n",
        "            done, winner = env.get_game_status()\n",
        "            if done:\n",
        "                if winner == 0: draws += 1\n",
        "                elif (winner == 1 and model_is_player1) or (winner == -1 and not model_is_player1): wins += 1\n",
        "                else: losses += 1\n",
        "                break\n",
        "            is_model_turn = (env.current_player == 1 and model_is_player1) or (env.current_player == -1 and not model_is_player1)\n",
        "            action = np.argmax(mcts.search(env)) if is_model_turn else baseline_agent_func(env)\n",
        "            if action not in env.get_legal_actions(): action = random.choice(env.get_legal_actions())\n",
        "            env.step(action)\n",
        "    return wins, draws, losses\n",
        "\n",
        "def enhanced_evaluate_model(model, device, num_games_per_opponent=50):\n",
        "    print(\"\\nğŸ† === MODEL EVALUATION ===\")\n",
        "    def run_eval(agent_name, agent_func):\n",
        "        print(f\"Evaluating against {agent_name} Agent ({num_games_per_opponent} games)...\")\n",
        "        wins, draws, losses = evaluate_against_baseline(model, device, agent_func, num_games_per_opponent)\n",
        "        total = wins + draws + losses\n",
        "        winrate = wins / total if total > 0 else 0\n",
        "        print(f\"  vs {agent_name}: {wins}W - {draws}D - {losses}L (Win Rate: {winrate:.1%})\")\n",
        "        return (wins + 0.5 * draws) / total if total > 0 else 0\n",
        "\n",
        "    random_score = run_eval(\"Random\", lambda env: random.choice(env.get_legal_actions()) if env.get_legal_actions() else 0)\n",
        "    # Re-implement greedy logic here for simplicity\n",
        "    def greedy_agent(env: MakNeebRLEnv):\n",
        "        actions = env.get_legal_actions()\n",
        "        if not actions: return 0\n",
        "        best_action, max_captures = actions[0], -1\n",
        "        for action in actions:\n",
        "            test_env = env.copy()\n",
        "            board_before = np.sum(np.abs(test_env.board))\n",
        "            test_env.step(action)\n",
        "            captures = board_before - np.sum(np.abs(test_env.board))\n",
        "            if captures > max_captures: max_captures, best_action = captures, action\n",
        "        return best_action if max_captures > 0 else random.choice(actions)\n",
        "\n",
        "    greedy_score = run_eval(\"Greedy\", greedy_agent)\n",
        "    strength = (random_score * 0.4 + greedy_score * 0.6) * 1000\n",
        "    print(f\"  Strength Score: {strength:.1f} / 1000\")\n",
        "    return {'strength': strength, 'random_winrate': random_score, 'greedy_winrate': greedy_score}\n"
      ],
      "metadata": {
        "id": "QmHfIMKzUeRP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== CHECKPOINTING  ==========\n",
        "def save_checkpoint(iteration, model, optimizer, replay_buffer, path):\n",
        "    torch.save({\n",
        "        'iteration': iteration,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'replay_buffer': list(replay_buffer)\n",
        "    }, path)\n",
        "    print(f\"ğŸ’¾ Checkpoint saved to {path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, path, device):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return 0, deque(maxlen=30000)\n",
        "    try:\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        replay_buffer = deque(checkpoint['replay_buffer'], maxlen=30000)\n",
        "        start_iter = checkpoint.get('iteration', -1) + 1\n",
        "        print(f\"âœ… Checkpoint loaded from iteration {start_iter - 1}. Replay buffer size: {len(replay_buffer)}\")\n",
        "        return start_iter, replay_buffer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
        "        return 0, deque(maxlen=30000)\n"
      ],
      "metadata": {
        "id": "n_Ux-gXRlsT7"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MAIN TRAINING LOOP ==========\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    TOTAL_GAMES = 10000\n",
        "    GAMES_PER_ITER = 48\n",
        "    NUM_WORKERS = 6  # *** à¸ˆà¸³à¸™à¸§à¸™ Worker à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰ *** (à¸›à¸£à¸±à¸šà¹„à¸”à¹‰à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ 4-8)\n",
        "    MCTS_SIMS = 80\n",
        "    C_PUCT = 1.5\n",
        "    INITIAL_LR = 0.001\n",
        "    BATCH_SIZE_TRAIN = 256\n",
        "    EPOCHS = 3\n",
        "    REPLAY_SIZE = 40000\n",
        "    CHECKPOINT_PATH = \"/content/drive/MyDrive/AlphaZero_Backups\"\n",
        "    ENTROPY_WEIGHT = 0.01\n",
        "    TEMPERATURE = 1.0\n",
        "    NOISE_ALPHA = 0.3\n",
        "    EVAL_GAMES = 40\n",
        "    EVAL_INTERVAL = 3\n",
        "\n",
        "    # Setup\n",
        "    try:\n",
        "        mp.set_start_method('spawn', force=True)\n",
        "        print(\"Multiprocessing start method set to 'spawn'.\")\n",
        "    except RuntimeError:\n",
        "        pass # Already set\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Main process training on {device}\")\n",
        "\n",
        "    model = MakNeebNet().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=INITIAL_LR, weight_decay=1e-5)\n",
        "    replay = deque(maxlen=REPLAY_SIZE)\n",
        "\n",
        "    start_iter, replay = load_checkpoint(model, optimizer, CHECKPOINT_PATH, device)\n",
        "\n",
        "    mcts_params = {\n",
        "        'mcts_sims': MCTS_SIMS,\n",
        "        'c_puct': C_PUCT,\n",
        "        'temperature': TEMPERATURE,\n",
        "        'noise_alpha': NOISE_ALPHA\n",
        "    }\n",
        "\n",
        "    num_iters = TOTAL_GAMES // GAMES_PER_ITER\n",
        "    for it in range(start_iter, num_iters):\n",
        "        print(f\"\\n{'='*25} Iteration {it}/{num_iters} {'='*25}\")\n",
        "\n",
        "        # --- Self-play phase ---\n",
        "        self_play_start = time.perf_counter()\n",
        "        training_examples, game_stats = parallel_self_play(\n",
        "            model, GAMES_PER_ITER, NUM_WORKERS, mcts_params\n",
        "        )\n",
        "        replay.extend(training_examples)\n",
        "        self_play_time = time.perf_counter() - self_play_start\n",
        "\n",
        "        # --- Logging ---\n",
        "        player1_wins = sum(1 for s in game_stats if s['winner'] == 1)\n",
        "        winrate = player1_wins / len(game_stats) if game_stats else 0\n",
        "        avg_len = np.mean([s['game_length'] for s in game_stats]) if game_stats else 0\n",
        "        print(f\"\\nğŸ“Š Self-Play Stats ({self_play_time:.1f}s):\")\n",
        "        print(f\"  P1 Win Rate: {winrate:.2%} | Avg Len: {avg_len:.1f} | New Examples: {len(training_examples)} | Buffer: {len(replay)}/{REPLAY_SIZE}\")\n",
        "\n",
        "        # --- Training phase ---\n",
        "        if len(replay) >= BATCH_SIZE_TRAIN:\n",
        "            print(\"\\nğŸ§  Training model...\")\n",
        "            training_start = time.perf_counter()\n",
        "            enhanced_train_model(model, replay, optimizer, device, BATCH_SIZE_TRAIN, EPOCHS, ENTROPY_WEIGHT)\n",
        "            print(f\"  Training Time: {time.perf_counter() - training_start:.1f}s\")\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸ Replay buffer too small for training ({len(replay)}/{BATCH_SIZE_TRAIN}). Skipping.\")\n",
        "\n",
        "        # --- Evaluation phase ---\n",
        "        if (it + 1) % EVAL_INTERVAL == 0 or it == num_iters - 1:\n",
        "            enhanced_evaluate_model(model, device, EVAL_GAMES)\n",
        "\n",
        "        save_checkpoint(it, model, optimizer, replay, CHECKPOINT_PATH)\n",
        "        gc.collect()\n"
      ],
      "metadata": {
        "id": "icz1wN74UiUR"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmyVhsdtWGkU",
        "outputId": "a9ec57a8-3ccf-490f-e10a-6e037b591d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiprocessing start method set to 'spawn'.\n",
            "Main process training on cuda\n",
            "Error loading checkpoint: [Errno 21] Is a directory: '/content/drive/MyDrive/AlphaZero_Backups'. Starting from scratch.\n",
            "\n",
            "========================= Iteration 0/208 =========================\n",
            "ğŸš€ Launching 6 parallel workers for 48 games...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S33BvuwqedlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
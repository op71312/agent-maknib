{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t07pJwjHVbeM",
        "outputId": "f95aa43f-3fc2-461f-c6d7-b0cca93f7b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hQMssVftVX6i"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import datetime\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import csv\n",
        "from numpy.core.multiarray import _reconstruct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RAEbnZBhiyu",
        "outputId": "ce2acb4a-48c9-4262-fa04-64dc7e001b77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0x_fr9cGWtsW"
      },
      "outputs": [],
      "source": [
        "# ========== ENVIRONMENT ==========\n",
        "class MakNeebRLEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(MakNeebRLEnv, self).__init__()\n",
        "        self.board_size = 8\n",
        "        self.action_space = spaces.Discrete(self.board_size * self.board_size * self.board_size * self.board_size)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.board_size, self.board_size), dtype=np.int8)\n",
        "        self.reset()\n",
        "\n",
        "    def _encode_action(self, from_row, from_col, to_row, to_col):\n",
        "        return from_row * (8*8*8) + from_col * (8*8) + to_row * 8 + to_col\n",
        "\n",
        "    def _decode_action(self, action):\n",
        "        from_row = action // (8*8*8)\n",
        "        action %= (8*8*8)\n",
        "        from_col = action // (8*8)\n",
        "        action %= (8*8)\n",
        "        to_row = action // 8\n",
        "        to_col = action % 8\n",
        "        return (from_row, from_col), (to_row, to_col)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int8)\n",
        "        self.board[0, :] = 1\n",
        "        self.board[7, :] = -1\n",
        "        self.current_player = 1\n",
        "        self.turns_without_capture = 0\n",
        "        self.max_turns_without_capture = 50\n",
        "        return self.board.copy(), {\"current_player\": self.current_player}\n",
        "\n",
        "    def get_legal_actions(self):\n",
        "        legal_actions = []\n",
        "        for r_from in range(self.board_size):\n",
        "            for c_from in range(self.board_size):\n",
        "                if self.board[r_from, c_from] == self.current_player:\n",
        "                    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                        for i in range(1, self.board_size):\n",
        "                            r_to, c_to = r_from + dr * i, c_from + dc * i\n",
        "                            if not (0 <= r_to < self.board_size and 0 <= c_to < self.board_size):\n",
        "                                break\n",
        "                            if self.board[r_to, c_to] != 0:\n",
        "                                break\n",
        "                            action = self._encode_action(r_from, c_from, r_to, c_to)\n",
        "                            legal_actions.append(action)\n",
        "        return legal_actions\n",
        "\n",
        "    def step(self, action):\n",
        "        legal_actions = self.get_legal_actions()\n",
        "        if action not in legal_actions:\n",
        "            return self.board.copy(), -10.0, True, False, {\"error\": \"Illegal move\"}\n",
        "\n",
        "        (from_row, from_col), (to_row, to_col) = self._decode_action(action)\n",
        "        self.board[to_row, to_col] = self.current_player\n",
        "        self.board[from_row, from_col] = 0\n",
        "\n",
        "        captured_count = self._check_and_capture(to_row, to_col)\n",
        "        reward = captured_count * 2\n",
        "\n",
        "        if captured_count > 0:\n",
        "            self.turns_without_capture = 0\n",
        "        else:\n",
        "            self.turns_without_capture += 1\n",
        "\n",
        "        done, winner = self.get_game_status()\n",
        "        if done:\n",
        "            if winner == self.current_player:\n",
        "                reward += 20\n",
        "            elif winner == -self.current_player:\n",
        "                reward -= 20\n",
        "            elif winner == 0:\n",
        "                reward += 5\n",
        "\n",
        "        self.current_player *= -1\n",
        "        info = {\"current_player\": self.current_player, \"captured\": captured_count > 0}\n",
        "        return self.board.copy(), float(reward), done, False, info\n",
        "\n",
        "    def _check_and_capture(self, r, c):\n",
        "        total_captured = 0\n",
        "        opponent = -self.current_player\n",
        "\n",
        "        for dr, dc in [(1, 0), (0, 1)]:\n",
        "            line_pieces = []\n",
        "            if dr == 1:\n",
        "                for i in range(self.board_size):\n",
        "                    if self.board[i, c] != 0:\n",
        "                        line_pieces.append({'player': self.board[i, c], 'pos': (i, c)})\n",
        "            else:\n",
        "                for i in range(self.board_size):\n",
        "                    if self.board[r, i] != 0:\n",
        "                        line_pieces.append({'player': self.board[r, i], 'pos': (r, i)})\n",
        "\n",
        "            if len(line_pieces) < 2:\n",
        "                continue\n",
        "\n",
        "            captured_in_this_line = set()\n",
        "\n",
        "            # Direct captures (sandwich pattern)\n",
        "            for i in range(len(line_pieces) - 2):\n",
        "                p1, p2, p3 = line_pieces[i], line_pieces[i+1], line_pieces[i+2]\n",
        "                if (p1['player'] == self.current_player and\n",
        "                    p3['player'] == self.current_player and\n",
        "                    p2['player'] == opponent):\n",
        "                    captured_in_this_line.add(p2['pos'])\n",
        "\n",
        "            # Multiple captures between pieces\n",
        "            my_indices = [i for i, p in enumerate(line_pieces) if p['player'] == self.current_player]\n",
        "            if len(my_indices) >= 2:\n",
        "                for i in range(len(my_indices) - 1):\n",
        "                    start_idx, end_idx = my_indices[i], my_indices[i+1]\n",
        "                    if end_idx > start_idx + 1:\n",
        "                        is_all_opponent = all(line_pieces[k]['player'] == opponent\n",
        "                                            for k in range(start_idx + 1, end_idx))\n",
        "                        if is_all_opponent:\n",
        "                            for k in range(start_idx + 1, end_idx):\n",
        "                                captured_in_this_line.add(line_pieces[k]['pos'])\n",
        "\n",
        "            # Apply captures\n",
        "            for pos_r, pos_c in captured_in_this_line:\n",
        "                self.board[pos_r, pos_c] = 0\n",
        "                total_captured += 1\n",
        "\n",
        "        return total_captured\n",
        "\n",
        "    def get_game_status(self):\n",
        "        player1_pieces = np.sum(self.board == 1)\n",
        "        player_minus_1_pieces = np.sum(self.board == -1)\n",
        "\n",
        "        if player1_pieces == 0:\n",
        "            return True, -1\n",
        "        if player_minus_1_pieces == 0:\n",
        "            return True, 1\n",
        "        if not self.get_legal_actions_for_player(-self.current_player):\n",
        "            return True, self.current_player\n",
        "\n",
        "        if self.turns_without_capture >= self.max_turns_without_capture:\n",
        "            if player1_pieces > player_minus_1_pieces:\n",
        "                return True, 1\n",
        "            elif player_minus_1_pieces > player1_pieces:\n",
        "                return True, -1\n",
        "            else:\n",
        "                return True, 0\n",
        "\n",
        "        return False, 0\n",
        "\n",
        "    def get_legal_actions_for_player(self, player):\n",
        "        for r_from in range(self.board_size):\n",
        "            for c_from in range(self.board_size):\n",
        "                if self.board[r_from, c_from] == player:\n",
        "                    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
        "                        r_to, c_to = r_from + dr, c_from + dc\n",
        "                        if (0 <= r_to < self.board_size and\n",
        "                            0 <= c_to < self.board_size and\n",
        "                            self.board[r_to, c_to] == 0):\n",
        "                            return [1]\n",
        "        return []\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = MakNeebRLEnv()\n",
        "        new_env.board = self.board.copy()\n",
        "        new_env.current_player = self.current_player\n",
        "        new_env.turns_without_capture = self.turns_without_capture\n",
        "        return new_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z1EraAMSVeVv"
      },
      "outputs": [],
      "source": [
        "# ========== NEURAL NETWORK ==========\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += residual\n",
        "        return F.relu(out)\n",
        "\n",
        "class MakNeebNet(nn.Module):\n",
        "    def __init__(self, board_size=8, num_res_blocks=5, num_channels=64, action_size=4096):\n",
        "        super(MakNeebNet, self).__init__()\n",
        "        self.conv_in = nn.Conv2d(1, num_channels, kernel_size=3, padding=1)\n",
        "        self.bn_in = nn.BatchNorm2d(num_channels)\n",
        "        self.res_blocks = nn.ModuleList([ResNetBlock(num_channels) for _ in range(num_res_blocks)])\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv2d(num_channels, 2, kernel_size=1)\n",
        "        self.policy_bn = nn.BatchNorm2d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_size * board_size, action_size)\n",
        "\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv2d(num_channels, 1, kernel_size=1)\n",
        "        self.value_bn = nn.BatchNorm1d(1 * board_size * board_size)\n",
        "        self.value_fc1 = nn.Linear(1 * board_size * board_size, 256)\n",
        "        self.value_fc2 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
        "\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Policy head\n",
        "        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        policy = policy.view(policy.size(0), -1)\n",
        "        policy = self.policy_fc(policy)\n",
        "\n",
        "        # Value head\n",
        "        value = F.relu(self.value_conv(x))\n",
        "        value = value.view(value.size(0), -1)\n",
        "        value = self.value_bn(value)\n",
        "        value = F.relu(self.value_fc1(value))\n",
        "        value = torch.tanh(self.value_fc2(value))\n",
        "\n",
        "        return F.log_softmax(policy, dim=1), value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UoNM-58PWz0y"
      },
      "outputs": [],
      "source": [
        "# ========== MCTS ==========\n",
        "class MCTSNode:\n",
        "    def __init__(self, parent=None, prior_p=1.0):\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0\n",
        "        self.prior_p = prior_p\n",
        "\n",
        "    def expand(self, action_priors):\n",
        "        for action, prior in action_priors:\n",
        "            if action not in self.children:\n",
        "                self.children[action] = MCTSNode(parent=self, prior_p=prior)\n",
        "\n",
        "    def select(self, c_puct):\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            score = self.get_ucb_score(child, c_puct)\n",
        "            if score > best_score:\n",
        "                best_score, best_action, best_child = score, action, child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def get_ucb_score(self, child, c_puct):\n",
        "        q_value = child.value()\n",
        "        u_value = c_puct * child.prior_p * math.sqrt(self.visit_count) / (1 + child.visit_count)\n",
        "        return q_value + u_value\n",
        "\n",
        "    def value(self):\n",
        "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0\n",
        "\n",
        "    def backpropagate(self, value):\n",
        "        self.visit_count += 1\n",
        "        self.value_sum += value\n",
        "        if self.parent:\n",
        "            self.parent.backpropagate(-value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, device, c_puct=1.5, num_simulations=100):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.c_puct = c_puct\n",
        "        self.num_simulations = num_simulations\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def search(self, env):\n",
        "        root = MCTSNode()\n",
        "\n",
        "        # Get initial policy and value\n",
        "        initial_board_tensor = torch.tensor(\n",
        "            env.board * env.current_player,\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(0).to(self.device)\n",
        "\n",
        "        log_policy, value = self.model(initial_board_tensor)\n",
        "        policy = torch.exp(log_policy).squeeze(0).cpu().numpy()\n",
        "\n",
        "        legal_actions = env.get_legal_actions()\n",
        "        if not legal_actions:\n",
        "            return np.ones(env.action_space.n) / env.action_space.n, value.item()\n",
        "\n",
        "        action_priors = [(action, policy[action]) for action in legal_actions]\n",
        "        root.expand(action_priors)\n",
        "\n",
        "        # Run simulations\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            sim_env = env.copy()\n",
        "\n",
        "            # Selection\n",
        "            while node.children:\n",
        "                action, node = node.select(self.c_puct)\n",
        "                sim_env.step(action)\n",
        "\n",
        "            # Evaluation\n",
        "            done, winner = sim_env.get_game_status()\n",
        "            value = 0\n",
        "\n",
        "            if not done:\n",
        "                # Expansion and evaluation\n",
        "                board_tensor = torch.tensor(\n",
        "                    sim_env.board * sim_env.current_player,\n",
        "                    dtype=torch.float32\n",
        "                ).unsqueeze(0).to(self.device)\n",
        "\n",
        "                log_policy, value_tensor = self.model(board_tensor)\n",
        "                policy = torch.exp(log_policy).squeeze(0).cpu().numpy()\n",
        "                value = value_tensor.item()\n",
        "\n",
        "                legal_actions = sim_env.get_legal_actions()\n",
        "                if legal_actions:\n",
        "                    action_priors = [(action, policy[action]) for action in legal_actions]\n",
        "                    node.expand(action_priors)\n",
        "            else:\n",
        "                # Terminal node\n",
        "                if winner != 0:\n",
        "                    value = 1 if winner == sim_env.current_player else -1\n",
        "\n",
        "            # Backpropagation\n",
        "            node.backpropagate(-value)\n",
        "\n",
        "        # Return visit count distribution and root value\n",
        "        visit_counts = np.array([\n",
        "            root.children.get(a, MCTSNode()).visit_count\n",
        "            for a in range(env.action_space.n)\n",
        "        ])\n",
        "\n",
        "        if visit_counts.sum() == 0:\n",
        "            return np.ones(env.action_space.n) / env.action_space.n, root.value()\n",
        "\n",
        "        action_probs = visit_counts / visit_counts.sum()\n",
        "        return action_probs, root.value()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NGIfYhGji3wq"
      },
      "outputs": [],
      "source": [
        "def calculate_training_metrics(model, batch_data, device):\n",
        "    \"\"\"Calculate additional training metrics\"\"\"\n",
        "    boards, target_policies, target_values = batch_data\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_policies, predicted_values = model(boards)\n",
        "        policies = torch.exp(log_policies)\n",
        "\n",
        "        # Entropy\n",
        "        entropy = -(policies * log_policies).sum(dim=1).mean()\n",
        "\n",
        "        # KL Divergence\n",
        "        kl_div = torch.sum(target_policies * torch.log(target_policies / (policies + 1e-8)), dim=1).mean()\n",
        "\n",
        "        # Value prediction bias\n",
        "        value_bias = (predicted_values.squeeze() - target_values).mean()\n",
        "\n",
        "        # Learning rate (from optimizer)\n",
        "        current_lr = None\n",
        "\n",
        "    return {\n",
        "        'entropy': entropy.item(),\n",
        "        'approx_kl': kl_div.item(),\n",
        "        'value_bias': value_bias.item(),\n",
        "        'current_lr': current_lr\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lo7k9ccnW5u1"
      },
      "outputs": [],
      "source": [
        "# ========== SELF-PLAY WORKER (CPU-only) ==========\n",
        "def self_play_worker(model_weights, mcts_simulations, c_puct):\n",
        "    \"\"\"CPU-only self-play worker\"\"\"\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    # Create model and load weights\n",
        "    model = MakNeebNet(action_size=4096).to(device)\n",
        "    model.load_state_dict(model_weights)\n",
        "    model.eval()\n",
        "\n",
        "    # Create environment and MCTS\n",
        "    env = MakNeebRLEnv()\n",
        "    mcts = MCTS(model, device, c_puct, mcts_simulations)\n",
        "\n",
        "    # Play one game\n",
        "    game_history = []\n",
        "    move_history = []  # เพิ่ม: เก็บประวัติการเดิน\n",
        "    current_player = 1\n",
        "    player_moves = {1: 0, -1: 0}  # เพิ่ม: นับจำนวนการเดินของแต่ละฝั่ง\n",
        "    player_times = {1: 0.0, -1: 0.0}  # เพิ่ม: เวลาที่ใช้ของแต่ละฝั่ง\n",
        "\n",
        "    while True:\n",
        "        move_start_time = time.time()\n",
        "\n",
        "        # Get board from current player's perspective\n",
        "        board = env.board * env.current_player\n",
        "\n",
        "        # Get action probabilities and value from MCTS\n",
        "        action_probs, mcts_value = mcts.search(env)\n",
        "\n",
        "        # Store training data\n",
        "        game_history.append((board.copy(), action_probs.copy(), current_player))\n",
        "\n",
        "        # Sample action and make move\n",
        "        action = np.random.choice(len(action_probs), p=action_probs)\n",
        "\n",
        "        # เพิ่ม: บันทึกการเดิน\n",
        "        (from_row, from_col), (to_row, to_col) = env._decode_action(action)\n",
        "        move_history.append({\n",
        "            'player': current_player,\n",
        "            'from_pos': (from_row, from_col),\n",
        "            'to_pos': (to_row, to_col),\n",
        "            'action': action\n",
        "        })\n",
        "\n",
        "        _, _, done, _, _ = env.step(action)\n",
        "\n",
        "        # เพิ่ม: อัพเดทจำนวนการเดินและเวลา\n",
        "        move_time = time.time() - move_start_time\n",
        "        player_moves[current_player] += 1\n",
        "        player_times[current_player] += move_time\n",
        "\n",
        "        current_player *= -1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Assign final rewards\n",
        "    _, winner = env.get_game_status()\n",
        "\n",
        "    # เพิ่ม: คำนวณหมากที่เหลือ\n",
        "    final_pieces = {\n",
        "        1: np.sum(env.board == 1),\n",
        "        -1: np.sum(env.board == -1)\n",
        "    }\n",
        "\n",
        "    training_examples = []\n",
        "    for board, action_probs, player in game_history:\n",
        "        if winner == 0:\n",
        "            reward = 0.0\n",
        "        else:\n",
        "            reward = 1.0 if winner == player else -1.0\n",
        "        training_examples.append((board, action_probs, reward))\n",
        "\n",
        "    # เพิ่ม: ส่งข้อมูลเกมกลับด้วย\n",
        "    game_data = {\n",
        "        'winner': winner,\n",
        "        'total_moves': len(move_history),\n",
        "        'player_1_moves': player_moves[1],\n",
        "        'player_minus1_moves': player_moves[-1],\n",
        "        'player_1_time': player_times[1],\n",
        "        'player_minus1_time': player_times[-1],\n",
        "        'final_pieces_p1': final_pieces[1],\n",
        "        'final_pieces_p_1': final_pieces[-1],\n",
        "        'move_history': move_history\n",
        "    }\n",
        "\n",
        "    return training_examples, game_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oPdx4um-W7rr"
      },
      "outputs": [],
      "source": [
        "# ========== TRAINING FUNCTIONS ==========\n",
        "def save_checkpoint(iteration, model, optimizer, replay_buffer, checkpoint_path):\n",
        "    # Save normally\n",
        "    torch.save({\n",
        "        'iteration': iteration,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'replay_buffer': list(replay_buffer)\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    # Auto-backup every 5 iterations\n",
        "    if iteration % 5 == 0:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "            backup_dir = \"/content/drive/MyDrive/AlphaZero_Backups\"\n",
        "            os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "            # Timestamped backup\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            backup_path = f\"{backup_dir}/checkpoint_iter_{iteration}_{timestamp}.pth\"\n",
        "            shutil.copy2(checkpoint_path, backup_path)\n",
        "\n",
        "            # Latest backup\n",
        "            latest_path = f\"{backup_dir}/checkpoint_latest.pth\"\n",
        "            shutil.copy2(checkpoint_path, latest_path)\n",
        "\n",
        "            print(f\"✅ Backed up checkpoint to Google Drive: {backup_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Google Drive checkpoint backup failed: {e}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
        "    \"\"\"Load training checkpoint\"\"\"\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(\"No checkpoint found, starting from scratch\")\n",
        "        return 0, deque(maxlen=50000)\n",
        "\n",
        "    # Load checkpoint with weights_only=False to allow loading non-tensor objects\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    replay_buffer = deque(checkpoint['replay_buffer'], maxlen=50000)\n",
        "\n",
        "    print(f\"Loaded checkpoint from iteration {checkpoint['iteration']}\")\n",
        "    return checkpoint['iteration'] + 1, replay_buffer\n",
        "\n",
        "\n",
        "def train_model(model, replay_buffer, optimizer, device, batch_size=128, num_epochs=10):\n",
        "    \"\"\"Train the model on replay buffer data\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    buffer_list = list(replay_buffer)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        random.shuffle(buffer_list)\n",
        "        total_loss = 0\n",
        "        policy_loss_sum = 0\n",
        "        value_loss_sum = 0\n",
        "        entropy_sum = 0\n",
        "        kl_sum = 0\n",
        "        value_acc_sum = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i in range(0, len(buffer_list) - batch_size + 1, batch_size):\n",
        "            batch = buffer_list[i:i + batch_size]\n",
        "\n",
        "            boards, target_policies, target_values = zip(*batch)\n",
        "\n",
        "            # Convert to tensors\n",
        "            boards = torch.tensor(np.array(boards), dtype=torch.float32).to(device)\n",
        "            target_policies = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)\n",
        "            target_values = torch.tensor(target_values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            log_policies, predicted_values = model(boards)\n",
        "\n",
        "            # Calculate losses\n",
        "            policy_loss = -(target_policies * log_policies).sum(dim=1).mean()\n",
        "            value_loss = F.mse_loss(predicted_values, target_values)\n",
        "            total_loss_batch = policy_loss + value_loss\n",
        "\n",
        "            # Calculate metrics\n",
        "            policy = torch.exp(log_policies)\n",
        "            entropy = -(policy * log_policies).sum(dim=1).mean()\n",
        "            kl = (target_policies * (torch.log(target_policies + 1e-10) - log_policies)).sum(dim=1).mean()\n",
        "            value_acc = (predicted_values.round() == target_values.round()).float().mean()\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss_batch.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_batch.item()\n",
        "            policy_loss_sum += policy_loss.item()\n",
        "            value_loss_sum += value_loss.item()\n",
        "            entropy_sum += entropy.item()\n",
        "            kl_sum += kl.item()\n",
        "            value_acc_sum += value_acc.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        if num_batches > 0:\n",
        "            avg_loss = total_loss / num_batches\n",
        "            avg_policy_loss = policy_loss_sum / num_batches\n",
        "            avg_value_loss = value_loss_sum / num_batches\n",
        "            avg_entropy = entropy_sum / num_batches\n",
        "            avg_kl = kl_sum / num_batches\n",
        "            avg_value_acc = value_acc_sum / num_batches\n",
        "            print(f\"  Epoch {epoch + 1}/{num_epochs}, Avg Loss: {avg_loss:.4f}, Policy Loss: {avg_policy_loss:.4f}, Value Loss: {avg_value_loss:.4f}, Entropy: {avg_entropy:.4f}, KL: {avg_kl:.6f}, Value Acc: {avg_value_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HoRPrXUVmzk1"
      },
      "outputs": [],
      "source": [
        "def calculate_iteration_stats(game_data_list):\n",
        "    \"\"\"คำนวณสถิติจากข้อมูลเกมในรอบนั้น\"\"\"\n",
        "    if not game_data_list:\n",
        "        return {\n",
        "            'win_rate_player1': 0.0,\n",
        "            'win_rate_player_minus1': 0.0,\n",
        "            'draw_rate': 0.0,\n",
        "            'avg_game_length': 0.0,\n",
        "            'avg_final_pieces_p1': 0.0,\n",
        "            'avg_final_pieces_p_1': 0.0\n",
        "        }\n",
        "\n",
        "    total_games = len(game_data_list)\n",
        "\n",
        "    # นับผลการแข่งขัน\n",
        "    player1_wins = sum(1 for game in game_data_list if game['winner'] == 1)\n",
        "    player_minus1_wins = sum(1 for game in game_data_list if game['winner'] == -1)\n",
        "    draws = sum(1 for game in game_data_list if game['winner'] == 0)\n",
        "\n",
        "    # คำนวณอัตราชนะ\n",
        "    win_rate_player1 = player1_wins / total_games\n",
        "    win_rate_player_minus1 = player_minus1_wins / total_games\n",
        "    draw_rate = draws / total_games\n",
        "\n",
        "    # คำนวณค่าเฉลี่ย\n",
        "    avg_game_length = sum(game['total_moves'] for game in game_data_list) / total_games\n",
        "    avg_final_pieces_p1 = sum(game['final_pieces_p1'] for game in game_data_list) / total_games\n",
        "    avg_final_pieces_p_1 = sum(game['final_pieces_p_1'] for game in game_data_list) / total_games\n",
        "\n",
        "    return {\n",
        "        'win_rate_player1': round(win_rate_player1, 4),\n",
        "        'win_rate_player_minus1': round(win_rate_player_minus1, 4),\n",
        "        'draw_rate': round(draw_rate, 4),\n",
        "        'avg_game_length': round(avg_game_length, 2),\n",
        "        'avg_final_pieces_p1': round(avg_final_pieces_p1, 2),\n",
        "        'avg_final_pieces_p_1': round(avg_final_pieces_p_1, 2)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yYa6LxkXb3GC"
      },
      "outputs": [],
      "source": [
        "def save_game_data_to_csv(game_data_list, iteration, filename_prefix=\"game_data\"):\n",
        "    \"\"\"บันทึกข้อมูลเกมลงไฟล์ CSV\"\"\"\n",
        "\n",
        "    # ข้อมูลสรุปเกม\n",
        "    game_summary = []\n",
        "    # ประวัติการเดินทั้งหมด\n",
        "    all_moves = []\n",
        "\n",
        "    for game_idx, game_data in enumerate(game_data_list):\n",
        "        # สรุปเกม\n",
        "        game_summary.append({\n",
        "            'iteration': iteration,\n",
        "            'game_number': game_idx + 1,\n",
        "            'winner': game_data['winner'],\n",
        "            'total_moves': game_data['total_moves'],\n",
        "            'player_1_moves': game_data['player_1_moves'],\n",
        "            'player_minus1_moves': game_data['player_minus1_moves'],\n",
        "            'player_1_time_seconds': round(game_data['player_1_time'], 3),\n",
        "            'player_minus1_time_seconds': round(game_data['player_minus1_time'], 3),\n",
        "            'final_pieces_p1': game_data['final_pieces_p1'],\n",
        "            'final_pieces_p_1': game_data['final_pieces_p_1']\n",
        "        })\n",
        "\n",
        "        # ประวัติการเดิน\n",
        "        for move_idx, move in enumerate(game_data['move_history']):\n",
        "            all_moves.append({\n",
        "                'iteration': iteration,\n",
        "                'game_number': game_idx + 1,\n",
        "                'move_number': move_idx + 1,\n",
        "                'player': move['player'],\n",
        "                'from_row': move['from_pos'][0],\n",
        "                'from_col': move['from_pos'][1],\n",
        "                'to_row': move['to_pos'][0],\n",
        "                'to_col': move['to_pos'][1],\n",
        "                'action_id': move['action']\n",
        "            })\n",
        "\n",
        "    # บันทึกไฟล์\n",
        "    pd.DataFrame(game_summary).to_csv(f\"{filename_prefix}_summary_iter_{iteration}.csv\", index=False)\n",
        "    pd.DataFrame(all_moves).to_csv(f\"{filename_prefix}_moves_iter_{iteration}.csv\", index=False)\n",
        "\n",
        "    return game_summary, all_moves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dW3Y4U1LW_Zl"
      },
      "outputs": [],
      "source": [
        "# ========== MAIN TRAINING LOOP ==========\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    NUM_GAMES_PER_ITERATION = 20  # Reduced for faster testing\n",
        "    NUM_TRAIN_EPOCHS = 5\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 1e-3\n",
        "    REPLAY_BUFFER_SIZE = 10000\n",
        "    MCTS_SIMULATIONS = 25  # Reduced for faster testing\n",
        "    C_PUCT = 1.5\n",
        "    CHECKPOINT_PATH = \"/content/drive/MyDrive/AlphaZero_Backups/checkpoint_latest.pth\"\n",
        "    SAVE_INTERVAL = 5 # Save every 5 iterations\n",
        "\n",
        "    # Setup device and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on {device}\")\n",
        "\n",
        "    model = MakNeebNet(action_size=4096).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_iteration, replay_buffer = load_checkpoint(model, optimizer, CHECKPOINT_PATH, device)\n",
        "\n",
        "    # Training statistics log\n",
        "    training_log = []\n",
        "\n",
        "    # เพิ่ม: ตัวแปรเก็บข้อมูลสะสมทุกรอบ\n",
        "    accumulated_game_summary = []\n",
        "    accumulated_game_moves = []\n",
        "    accumulated_iteration_stats = []\n",
        "\n",
        "    try:\n",
        "        for iteration in range(start_iteration, 50):  # Changed the end of the range to 50\n",
        "            iteration_start_time = time.time() # Define iteration_start_time here\n",
        "            print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "            # Self-play phase\n",
        "            print(f\"Generating {NUM_GAMES_PER_ITERATION} self-play games...\")\n",
        "\n",
        "            # Get model weights for workers (ensure CPU compatibility)\n",
        "            model_weights = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "            # Generate self-play games sequentially (removed Ray)\n",
        "            game_results = []\n",
        "            game_data_list = []  # เพิ่ม: เก็บข้อมูลเกม\n",
        "\n",
        "            for game_num in range(NUM_GAMES_PER_ITERATION):\n",
        "                game_start = time.time()\n",
        "                training_examples, game_data = self_play_worker(model_weights, MCTS_SIMULATIONS, C_PUCT)\n",
        "\n",
        "                replay_buffer.extend(training_examples)\n",
        "                game_data_list.append(game_data)\n",
        "\n",
        "                game_time = time.time() - game_start\n",
        "                print(f\"  Game {game_num + 1}: Time={game_time:.1f}s, Examples={len(training_examples)}, Winner={game_data['winner']}\")\n",
        "\n",
        "            print(f\"Replay buffer size: {len(replay_buffer)}\")\n",
        "\n",
        "            # เพิ่ม: คำนวณสถิติรอบนี้\n",
        "            iteration_stats = calculate_iteration_stats(game_data_list)\n",
        "\n",
        "            # Training phase\n",
        "            iteration_log = {\n",
        "                'iteration': iteration,\n",
        "                'win_rate_player1': iteration_stats['win_rate_player1'],\n",
        "                'win_rate_player_minus1': iteration_stats['win_rate_player_minus1'],\n",
        "                'draw_rate': iteration_stats['draw_rate'],\n",
        "                'avg_game_length': iteration_stats['avg_game_length'],\n",
        "                'avg_final_pieces_p1': iteration_stats['avg_final_pieces_p1'],\n",
        "                'avg_final_pieces_p_1': iteration_stats['avg_final_pieces_p_1'],\n",
        "                'new_training_examples': len(game_data_list) * len(training_examples) if training_examples else 0,\n",
        "                'replay_buffer_size': len(replay_buffer),\n",
        "                'iteration_time': 0.0\n",
        "            }\n",
        "\n",
        "            # เพิ่ม: เก็บสถิติรอบนี้\n",
        "            accumulated_iteration_stats.append(iteration_log)\n",
        "\n",
        "            # Log iteration results\n",
        "            training_log.append(iteration_log)\n",
        "\n",
        "            # เพิ่ม: เก็บข้อมูลเกมรอบนี้เข้าไปในข้อมูลสะสม\n",
        "            for game_idx, game_data in enumerate(game_data_list):\n",
        "                accumulated_game_summary.append({\n",
        "                    'iteration': iteration,\n",
        "                    'game_number': game_idx + 1,\n",
        "                    'winner': game_data['winner'],\n",
        "                    'total_moves': game_data['total_moves'],\n",
        "                    'player_1_moves': game_data['player_1_moves'],\n",
        "                    'player_minus1_moves': game_data['player_minus1_moves'],\n",
        "                    'player_1_time_seconds': round(game_data['player_1_time'], 3),\n",
        "                    'player_minus1_time_seconds': round(game_data['player_minus1_time'], 3),\n",
        "                    'final_pieces_p1': game_data['final_pieces_p1'],\n",
        "                    'final_pieces_p_1': game_data['final_pieces_p_1']\n",
        "                })\n",
        "\n",
        "                for move_idx, move in enumerate(game_data['move_history']):\n",
        "                    accumulated_game_moves.append({\n",
        "                        'iteration': iteration,\n",
        "                        'game_number': game_idx + 1,\n",
        "                        'move_number': move_idx + 1,\n",
        "                        'player': move['player'],\n",
        "                        'from_row': move['from_pos'][0],\n",
        "                        'from_col': move['from_pos'][1],\n",
        "                        'to_row': move['to_pos'][0],\n",
        "                        'to_col': move['to_pos'][1],\n",
        "                        'action_id': move['action']\n",
        "                    })\n",
        "\n",
        "            # Training phase\n",
        "            training_losses = []\n",
        "\n",
        "            if len(replay_buffer) >= BATCH_SIZE:\n",
        "                print(\"Training model...\")\n",
        "                training_start = time.time()\n",
        "                training_losses = train_model(model, replay_buffer, optimizer, device, BATCH_SIZE, NUM_TRAIN_EPOCHS)\n",
        "                training_time = time.time() - training_start\n",
        "                print(f\"Training completed in {training_time:.1f}s\")\n",
        "\n",
        "                if training_losses:\n",
        "                    # Calculate average metrics from training epochs\n",
        "                    avg_metrics = {\n",
        "                        'avg_policy_loss': np.mean([e['policy_loss'] for e in training_losses]),\n",
        "                        'avg_value_loss': np.mean([e['value_loss'] for e in training_losses]),\n",
        "                        'avg_entropy': np.mean([e['entropy'] for e in training_losses]),\n",
        "                        'avg_approx_kl': np.mean([e['approx_kl'] for e in training_losses]),\n",
        "                        'avg_value_acc': np.mean([e['value_acc'] for e in training_losses]),\n",
        "                    }\n",
        "                    iteration_log.update(avg_metrics)\n",
        "\n",
        "                    # Display summary\n",
        "                    print(f\"📊 TRAINING METRICS SUMMARY:\")\n",
        "                    print(f\"Avg Policy Loss: {avg_metrics['avg_policy_loss']:.4f}\")\n",
        "                    print(f\"Avg Value Loss: {avg_metrics['avg_value_loss']:.4f}\")\n",
        "                    print(f\"Avg Entropy: {avg_metrics['avg_entropy']:.4f}\")\n",
        "                    print(f\"Avg KL Divergence: {avg_metrics['avg_approx_kl']:.6f}\")\n",
        "                    print(f\"Avg Value Accuracy: {avg_metrics['avg_value_acc']:.4f}\")\n",
        "\n",
        "            # Calculate iteration statistics (including time)\n",
        "            iteration_end_time = time.time()\n",
        "            iteration_log['iteration_time'] = iteration_end_time - iteration_start_time\n",
        "\n",
        "            # Print summary (using data from iteration_log)\n",
        "            print(f\"\\n📊 ITERATION {iteration} SUMMARY:\")\n",
        "            print(f\"Buffer size: {iteration_log['replay_buffer_size']}\")\n",
        "            if 'avg_policy_loss' in iteration_log: # Check if training happened\n",
        "                print(f\"Avg Policy Loss: {iteration_log['avg_policy_loss']:.4f}, Avg Value Loss: {iteration_log['avg_value_loss']:.4f}\")\n",
        "                print(f\"Avg Entropy: {iteration_log['avg_entropy']:.4f}, Avg KL Divergence: {iteration_log['avg_approx_kl']:.6f}, Avg Value Accuracy: {iteration_log['avg_value_acc']:.4f}\")\n",
        "            print(f\"Time: {iteration_log['iteration_time']:.1f}s\")\n",
        "\n",
        "            # Save checkpoint and logs every SAVE_INTERVAL iterations OR at iteration 49 (special case)\n",
        "            if iteration % SAVE_INTERVAL == 0 or iteration == 49:\n",
        "                save_checkpoint(iteration, model, optimizer, replay_buffer, CHECKPOINT_PATH)\n",
        "\n",
        "                # บันทึกข้อมูลสะสม\n",
        "                if iteration == 49:\n",
        "                    # กรณีพิเศษ: iteration 49 (บันทึกข้อมูล iteration 46-49)\n",
        "                    range_start = 46\n",
        "                    range_end = 49\n",
        "                else:\n",
        "                    # กรณีปกติ: บันทึกตาม SAVE_INTERVAL\n",
        "                    range_start = iteration - SAVE_INTERVAL + 1\n",
        "                    range_end = iteration\n",
        "\n",
        "                # 1. บันทึกข้อมูลเกม\n",
        "                summary_filename = f\"game_data_summary_rounds_{range_start}_to_{range_end}.csv\"\n",
        "                moves_filename = f\"game_data_moves_rounds_{range_start}_to_{range_end}.csv\"\n",
        "                stats_filename = f\"iteration_stats_rounds_{range_start}_to_{range_end}.csv\"\n",
        "\n",
        "                pd.DataFrame(accumulated_game_summary).to_csv(summary_filename, index=False)\n",
        "                pd.DataFrame(accumulated_game_moves).to_csv(moves_filename, index=False)\n",
        "                pd.DataFrame(accumulated_iteration_stats).to_csv(stats_filename, index=False)\n",
        "\n",
        "                print(f\"📁 Saved game data: {len(accumulated_game_summary)} games, {len(accumulated_game_moves)} moves\")\n",
        "                print(f\"📁 Saved iteration stats: {len(accumulated_iteration_stats)} iterations\")\n",
        "\n",
        "                try:\n",
        "                    from google.colab import drive\n",
        "                    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "                    backup_dir = \"/content/drive/MyDrive/AlphaZero_Backups\"\n",
        "                    os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "                    # Backup files\n",
        "                    files_to_backup = [\n",
        "                        (summary_filename, f\"game_summary_rounds_{range_start}_to_{range_end}_{timestamp}.csv\"),\n",
        "                        (moves_filename, f\"game_moves_rounds_{range_start}_to_{range_end}_{timestamp}.csv\"),\n",
        "                        (stats_filename, f\"iteration_stats_rounds_{range_start}_to_{range_end}_{timestamp}.csv\")\n",
        "                    ]\n",
        "\n",
        "                    for local_file, backup_name in files_to_backup:\n",
        "                        if os.path.exists(local_file):\n",
        "                            backup_path = f\"{backup_dir}/{backup_name}\"\n",
        "                            shutil.copy2(local_file, backup_path)\n",
        "                            print(f\"✅ Backed up: {backup_path}\")\n",
        "\n",
        "                    # Save training log\n",
        "                    df_log = pd.DataFrame(training_log)\n",
        "                    log_backup_path = f\"{backup_dir}/training_log_iter_{iteration}_{timestamp}.csv\"\n",
        "                    df_log.to_csv(log_backup_path, index=False)\n",
        "                    print(f\"✅ Backed up training log to Google Drive: {log_backup_path}\")\n",
        "\n",
        "                    # Backup additional game files if they exist\n",
        "                    game_summary_file = f\"game_data_summary_iter_{iteration}.csv\"\n",
        "                    game_moves_file = f\"game_data_moves_iter_{iteration}.csv\"\n",
        "\n",
        "                    if os.path.exists(game_summary_file):\n",
        "                        shutil.copy2(game_summary_file, f\"{backup_dir}/game_summary_iter_{iteration}_{timestamp}.csv\")\n",
        "                    if os.path.exists(game_moves_file):\n",
        "                        shutil.copy2(game_moves_file, f\"{backup_dir}/game_moves_iter_{iteration}_{timestamp}.csv\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️  Google Drive log backup failed: {e}\")\n",
        "\n",
        "                # Clear accumulated data after saving\n",
        "                accumulated_game_summary = []\n",
        "                accumulated_game_moves = []\n",
        "                accumulated_iteration_stats = []\n",
        "\n",
        "            # GPU memory management\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Training interrupted by user\")\n",
        "    finally:\n",
        "        print(\"\\nTraining completed\")\n",
        "\n",
        "        # Save final training log\n",
        "        if training_log:\n",
        "            df_log = pd.DataFrame(training_log)\n",
        "            df_log.to_csv(\"final_training_log.csv\", index=False)\n",
        "            print(\"📁 Final training log saved to final_training_log.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46nCT2ZHXA8y",
        "outputId": "4280c650-39ab-44a6-df4c-753b0339a770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu\n",
            "Loaded checkpoint from iteration 45\n",
            "\n",
            "--- Iteration 46 ---\n",
            "Generating 20 self-play games...\n",
            "  Game 1: Time=9.1s, Examples=89, Winner=0\n",
            "  Game 2: Time=23.7s, Examples=220, Winner=-1\n",
            "  Game 3: Time=13.8s, Examples=59, Winner=1\n",
            "  Game 4: Time=19.2s, Examples=183, Winner=1\n",
            "  Game 5: Time=6.7s, Examples=54, Winner=-1\n",
            "  Game 6: Time=28.6s, Examples=264, Winner=-1\n",
            "  Game 7: Time=26.7s, Examples=245, Winner=1\n",
            "  Game 8: Time=16.5s, Examples=151, Winner=1\n",
            "  Game 9: Time=5.0s, Examples=50, Winner=0\n",
            "  Game 10: Time=20.9s, Examples=178, Winner=-1\n",
            "  Game 11: Time=4.9s, Examples=50, Winner=0\n",
            "  Game 12: Time=7.4s, Examples=58, Winner=-1\n",
            "  Game 13: Time=5.9s, Examples=57, Winner=1\n",
            "  Game 14: Time=15.3s, Examples=140, Winner=-1\n",
            "  Game 15: Time=39.0s, Examples=353, Winner=1\n",
            "  Game 16: Time=7.5s, Examples=64, Winner=0\n",
            "  Game 17: Time=11.9s, Examples=113, Winner=1\n",
            "  Game 18: Time=11.4s, Examples=111, Winner=1\n",
            "  Game 19: Time=18.7s, Examples=163, Winner=1\n",
            "  Game 20: Time=4.7s, Examples=50, Winner=0\n",
            "Replay buffer size: 50000\n",
            "Training model...\n",
            "  Epoch 1/5, Avg Loss: 1.7453, Policy Loss: 1.6541, Value Loss: 0.0913, Entropy: 1.6481, KL: 0.216896, Value Acc: 0.9251\n",
            "  Epoch 2/5, Avg Loss: 1.7115, Policy Loss: 1.6425, Value Loss: 0.0690, Entropy: 1.6444, KL: 0.205164, Value Acc: 0.9360\n",
            "  Epoch 3/5, Avg Loss: 1.6973, Policy Loss: 1.6360, Value Loss: 0.0613, Entropy: 1.6365, KL: 0.198511, Value Acc: 0.9425\n",
            "  Epoch 4/5, Avg Loss: 1.6852, Policy Loss: 1.6303, Value Loss: 0.0549, Entropy: 1.6298, KL: 0.192689, Value Acc: 0.9469\n",
            "  Epoch 5/5, Avg Loss: 1.6769, Policy Loss: 1.6253, Value Loss: 0.0516, Entropy: 1.6263, KL: 0.188017, Value Acc: 0.9492\n",
            "Training completed in 880.3s\n",
            "\n",
            "📊 ITERATION 46 SUMMARY:\n",
            "Buffer size: 50000\n",
            "Time: 1177.0s\n",
            "\n",
            "--- Iteration 47 ---\n",
            "Generating 20 self-play games...\n",
            "  Game 1: Time=13.6s, Examples=119, Winner=1\n",
            "  Game 2: Time=8.0s, Examples=80, Winner=-1\n",
            "  Game 3: Time=6.5s, Examples=50, Winner=0\n",
            "  Game 4: Time=26.7s, Examples=249, Winner=1\n",
            "  Game 5: Time=11.1s, Examples=110, Winner=1\n",
            "  Game 6: Time=15.4s, Examples=129, Winner=1\n",
            "  Game 7: Time=13.4s, Examples=120, Winner=-1\n",
            "  Game 8: Time=7.4s, Examples=64, Winner=0\n",
            "  Game 9: Time=8.9s, Examples=76, Winner=-1\n",
            "  Game 10: Time=4.8s, Examples=50, Winner=0\n",
            "  Game 11: Time=16.0s, Examples=146, Winner=0\n",
            "  Game 12: Time=6.5s, Examples=50, Winner=0\n",
            "  Game 13: Time=5.2s, Examples=50, Winner=0\n",
            "  Game 14: Time=27.9s, Examples=249, Winner=1\n",
            "  Game 15: Time=5.4s, Examples=50, Winner=0\n",
            "  Game 16: Time=12.8s, Examples=115, Winner=1\n",
            "  Game 17: Time=21.8s, Examples=201, Winner=-1\n",
            "  Game 18: Time=15.2s, Examples=137, Winner=1\n",
            "  Game 19: Time=5.9s, Examples=50, Winner=0\n",
            "  Game 20: Time=17.5s, Examples=156, Winner=-1\n",
            "Replay buffer size: 50000\n",
            "Training model...\n",
            "  Epoch 1/5, Avg Loss: 1.7244, Policy Loss: 1.6457, Value Loss: 0.0787, Entropy: 1.6426, KL: 0.210091, Value Acc: 0.9315\n",
            "  Epoch 2/5, Avg Loss: 1.6958, Policy Loss: 1.6337, Value Loss: 0.0621, Entropy: 1.6343, KL: 0.198094, Value Acc: 0.9406\n",
            "  Epoch 3/5, Avg Loss: 1.6847, Policy Loss: 1.6284, Value Loss: 0.0563, Entropy: 1.6282, KL: 0.192866, Value Acc: 0.9450\n",
            "  Epoch 4/5, Avg Loss: 1.6825, Policy Loss: 1.6273, Value Loss: 0.0552, Entropy: 1.6280, KL: 0.191682, Value Acc: 0.9458\n",
            "  Epoch 5/5, Avg Loss: 1.6682, Policy Loss: 1.6191, Value Loss: 0.0491, Entropy: 1.6199, KL: 0.183408, Value Acc: 0.9508\n",
            "Training completed in 881.5s\n",
            "\n",
            "📊 ITERATION 47 SUMMARY:\n",
            "Buffer size: 50000\n",
            "Time: 1131.4s\n",
            "\n",
            "--- Iteration 48 ---\n",
            "Generating 20 self-play games...\n",
            "  Game 1: Time=5.2s, Examples=50, Winner=0\n",
            "  Game 2: Time=16.0s, Examples=143, Winner=1\n",
            "  Game 3: Time=15.2s, Examples=137, Winner=1\n",
            "  Game 4: Time=7.6s, Examples=59, Winner=1\n",
            "  Game 5: Time=4.9s, Examples=50, Winner=0\n",
            "  Game 6: Time=20.7s, Examples=174, Winner=-1\n",
            "  Game 7: Time=9.0s, Examples=90, Winner=-1\n",
            "  Game 8: Time=15.6s, Examples=134, Winner=-1\n",
            "  Game 9: Time=14.7s, Examples=127, Winner=1\n",
            "  Game 10: Time=5.6s, Examples=50, Winner=0\n",
            "  Game 11: Time=5.0s, Examples=50, Winner=0\n",
            "  Game 12: Time=9.7s, Examples=80, Winner=-1\n",
            "  Game 13: Time=6.3s, Examples=59, Winner=1\n",
            "  Game 14: Time=6.4s, Examples=55, Winner=1\n",
            "  Game 15: Time=5.9s, Examples=59, Winner=1\n",
            "  Game 16: Time=13.7s, Examples=121, Winner=-1\n",
            "  Game 17: Time=10.2s, Examples=86, Winner=-1\n",
            "  Game 18: Time=17.0s, Examples=156, Winner=-1\n",
            "  Game 19: Time=19.7s, Examples=165, Winner=1\n",
            "  Game 20: Time=10.7s, Examples=92, Winner=-1\n",
            "Replay buffer size: 50000\n",
            "Training model...\n",
            "  Epoch 1/5, Avg Loss: 1.7120, Policy Loss: 1.6386, Value Loss: 0.0734, Entropy: 1.6355, KL: 0.202859, Value Acc: 0.9339\n",
            "  Epoch 2/5, Avg Loss: 1.6903, Policy Loss: 1.6308, Value Loss: 0.0595, Entropy: 1.6311, KL: 0.195124, Value Acc: 0.9425\n",
            "  Epoch 3/5, Avg Loss: 1.6801, Policy Loss: 1.6251, Value Loss: 0.0550, Entropy: 1.6256, KL: 0.189488, Value Acc: 0.9462\n",
            "  Epoch 4/5, Avg Loss: 1.6770, Policy Loss: 1.6240, Value Loss: 0.0530, Entropy: 1.6235, KL: 0.188158, Value Acc: 0.9474\n",
            "  Epoch 5/5, Avg Loss: 1.6724, Policy Loss: 1.6211, Value Loss: 0.0513, Entropy: 1.6212, KL: 0.185324, Value Acc: 0.9491\n",
            "Training completed in 887.7s\n",
            "\n",
            "📊 ITERATION 48 SUMMARY:\n",
            "Buffer size: 50000\n",
            "Time: 1106.7s\n",
            "\n",
            "--- Iteration 49 ---\n",
            "Generating 20 self-play games...\n",
            "  Game 1: Time=4.9s, Examples=50, Winner=0\n",
            "  Game 2: Time=24.5s, Examples=219, Winner=1\n",
            "  Game 3: Time=10.9s, Examples=96, Winner=-1\n",
            "  Game 4: Time=5.0s, Examples=50, Winner=0\n",
            "  Game 5: Time=17.1s, Examples=155, Winner=1\n",
            "  Game 6: Time=12.6s, Examples=113, Winner=1\n",
            "  Game 7: Time=22.2s, Examples=197, Winner=1\n",
            "  Game 8: Time=16.7s, Examples=155, Winner=1\n",
            "  Game 9: Time=23.6s, Examples=212, Winner=-1\n",
            "  Game 10: Time=13.3s, Examples=122, Winner=1\n",
            "  Game 11: Time=20.4s, Examples=190, Winner=0\n",
            "  Game 12: Time=6.7s, Examples=50, Winner=0\n",
            "  Game 13: Time=20.1s, Examples=188, Winner=-1\n",
            "  Game 14: Time=24.5s, Examples=226, Winner=0\n",
            "  Game 15: Time=5.0s, Examples=50, Winner=0\n",
            "  Game 16: Time=6.3s, Examples=50, Winner=0\n",
            "  Game 17: Time=27.1s, Examples=247, Winner=1\n",
            "  Game 18: Time=10.4s, Examples=99, Winner=1\n",
            "  Game 19: Time=5.7s, Examples=50, Winner=0\n",
            "  Game 20: Time=33.9s, Examples=318, Winner=1\n",
            "Replay buffer size: 50000\n",
            "Training model...\n",
            "  Epoch 1/5, Avg Loss: 1.7333, Policy Loss: 1.6451, Value Loss: 0.0882, Entropy: 1.6413, KL: 0.211451, Value Acc: 0.9272\n",
            "  Epoch 2/5, Avg Loss: 1.7102, Policy Loss: 1.6378, Value Loss: 0.0724, Entropy: 1.6377, KL: 0.204018, Value Acc: 0.9332\n",
            "  Epoch 3/5, Avg Loss: 1.6922, Policy Loss: 1.6304, Value Loss: 0.0618, Entropy: 1.6321, KL: 0.196389, Value Acc: 0.9400\n",
            "  Epoch 4/5, Avg Loss: 1.6804, Policy Loss: 1.6240, Value Loss: 0.0563, Entropy: 1.6250, KL: 0.190118, Value Acc: 0.9447\n",
            "  Epoch 5/5, Avg Loss: 1.6773, Policy Loss: 1.6223, Value Loss: 0.0550, Entropy: 1.6217, KL: 0.188469, Value Acc: 0.9456\n",
            "Training completed in 888.8s\n",
            "\n",
            "📊 ITERATION 49 SUMMARY:\n",
            "Buffer size: 50000\n",
            "Time: 1199.6s\n",
            "📁 Saved game data: 80 games, 9677 moves\n",
            "📁 Saved iteration stats: 4 iterations\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Backed up: /content/drive/MyDrive/AlphaZero_Backups/game_summary_rounds_46_to_49_20250622_052913.csv\n",
            "✅ Backed up: /content/drive/MyDrive/AlphaZero_Backups/game_moves_rounds_46_to_49_20250622_052913.csv\n",
            "✅ Backed up: /content/drive/MyDrive/AlphaZero_Backups/iteration_stats_rounds_46_to_49_20250622_052913.csv\n",
            "✅ Backed up training log to Google Drive: /content/drive/MyDrive/AlphaZero_Backups/training_log_iter_49_20250622_052913.csv\n",
            "\n",
            "Training completed\n",
            "📁 Final training log saved to final_training_log.csv\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# โหลดโมเดล\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MakNeebNet(action_size=4096).to(device)\n",
        "checkpoint_path = \"/content/drive/MyDrive/AlphaZero_Backups/checkpoint_latest.pth\" # ตรวจสอบพาธให้ถูกต้อง\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Model loaded successfully from checkpoint.\")\n",
        "    model.eval() # ตั้งค่าโมเดลเป็นโหมด evaluation\n",
        "except FileNotFoundError:\n",
        "    print(\"Checkpoint file not found. Cannot load model.\")\n",
        "    # จัดการกรณีที่ไม่พบไฟล์ checkpoint\n",
        "    exit()\n",
        "\n",
        "# สร้าง MCTS agent\n",
        "mcts_agent = MCTS(model, device, c_puct=1.5, num_simulations=400) # ใช้ simulations เยอะขึ้นเพื่อการเล่นจริง\n",
        "\n",
        "# สร้างสภาพแวดล้อมเกม\n",
        "env = MakNeebRLEnv()\n",
        "observation, info = env.reset()\n",
        "\n",
        "# เริ่มเล่นเกม\n",
        "print(\"Game Start!\")\n",
        "print(env.board) # แสดงกระดานเริ่มต้น\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    current_player = env.current_player\n",
        "    print(f\"\\nPlayer {current_player}'s turn\")\n",
        "\n",
        "    # ให้ MCTS Agent เลือกการเดิน\n",
        "    # ในการเล่นจริง อาจจะใช้ temperature = 0 เพื่อเลือกการเดินที่มี visit count สูงสุด\n",
        "    # แทนที่จะสุ่มตาม distribution เหมือนตอน self-play\n",
        "    action_probs, value = mcts_agent.search(env)\n",
        "\n",
        "    # เลือกการเดินที่มี probability สูงสุด\n",
        "    best_action = np.argmax(action_probs)\n",
        "    (from_row, from_col), (to_row, to_col) = env._decode_action(best_action)\n",
        "\n",
        "    print(f\"Agent chooses move: ({from_row}, {from_col}) -> ({to_row}, {to_col}) (Action ID: {best_action})\")\n",
        "\n",
        "    # ทำการเดินในสภาพแวดล้อม\n",
        "    observation, reward, done, truncated, info = env.step(best_action)\n",
        "\n",
        "    # แสดงกระดานปัจจุบัน\n",
        "    print(env.board)\n",
        "    print(f\"Reward: {reward}, Done: {done}\")\n",
        "\n",
        "    if done:\n",
        "        _, winner = env.get_game_status()\n",
        "        if winner == 0:\n",
        "            print(\"Game ended in a draw!\")\n",
        "        else:\n",
        "            print(f\"Player {winner} wins!\")\n",
        "        break\n",
        "\n",
        "# คุณสามารถเพิ่มโค้ดเพื่อเล่นกับผู้เล่นคนอื่น หรือเล่นโมเดลกับตัวเองได้ที่นี่"
      ],
      "metadata": {
        "id": "k5XAr3isASLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50861485-ee15-4fcc-9756-1b5c54926c25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from checkpoint.\n",
            "Game Start!\n",
            "[[ 1  1  1  1  1  1  1  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]]\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 6) -> (2, 6) (Action ID: 406)\n",
            "[[ 1  1  1  1  1  1  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 0) -> (3, 0) (Action ID: 3608)\n",
            "[[ 1  1  1  1  1  1  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1 -1 -1 -1 -1 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 5) -> (4, 5) (Action ID: 357)\n",
            "[[ 1  1  1  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1 -1 -1 -1 -1 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 1) -> (4, 1) (Action ID: 3681)\n",
            "[[ 1  1  1  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1 -1 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (2, 6) -> (3, 6) (Action ID: 1438)\n",
            "[[ 1  1  1  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0 -1  0  0  0  1  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1 -1 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 6) -> (4, 6) (Action ID: 4006)\n",
            "[[ 1  1  1  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0 -1  0  0  0  0 -1  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1  0 -1]]\n",
            "Reward: 2.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  1  1  1  1  0  0  1]\n",
            " [ 1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0 -1  0  0  0  0 -1  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1 -1 -1 -1  0 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 2) -> (5, 2) (Action ID: 3754)\n",
            "[[ 0  1  1  1  1  0  0  1]\n",
            " [ 1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 1) -> (1, 1) (Action ID: 73)\n",
            "[[ 0  0  1  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 0) -> (4, 0) (Action ID: 1568)\n",
            "[[ 0  0  1  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 2) -> (0, 1) (Action ID: 129)\n",
            "[[ 0  1  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0 -1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 7) -> (5, 7) (Action ID: 4079)\n",
            "[[ 0  1  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 1) -> (0, 0) (Action ID: 64)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0  0  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 4) -> (4, 4) (Action ID: 3876)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (0, 1) (Action ID: 1)\n",
            "[[ 0  1  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (5, 7) -> (2, 7) (Action ID: 3031)\n",
            "[[ 0  1  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 1) -> (0, 0) (Action ID: 64)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 7) -> (1, 7) (Action ID: 1487)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (3, 6) -> (3, 7) (Action ID: 1951)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0 -1  0 -1  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 2.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 6) -> (4, 7) (Action ID: 2471)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0 -1  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 1) -> (1, 4) (Action ID: 588)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0 -1  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 3) -> (6, 3) (Action ID: 3827)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0 -1  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (3, 0) (Action ID: 536)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0 -1  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0  0 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 4) -> (7, 4) (Action ID: 2364)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  1]\n",
            " [-1 -1  0  0  0  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (3, 0) -> (3, 6) (Action ID: 1566)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  1]\n",
            " [-1 -1  0  0  0  0  0 -1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (5, 2) -> (4, 2) (Action ID: 2722)\n",
            "[[ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  1]\n",
            " [-1 -1 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 3) -> (3, 3) (Action ID: 219)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  1  1]\n",
            " [-1 -1 -1  0  0  0  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1 -1  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 5) -> (4, 5) (Action ID: 3941)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  1  1]\n",
            " [-1 -1 -1  0  0 -1  0 -1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (3, 6) -> (5, 6) (Action ID: 1966)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  1]\n",
            " [-1 -1 -1  0  0 -1  0 -1]\n",
            " [ 0  0  0  0  0  0  1  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 0) -> (5, 0) (Action ID: 2088)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  1]\n",
            " [ 0 -1 -1  0  0 -1  0 -1]\n",
            " [-1  0  0  0  0  0  1  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (5, 6) -> (5, 7) (Action ID: 2991)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  1]\n",
            " [ 0 -1 -1  0  0 -1  0  0]\n",
            " [-1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 2.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 1) -> (3, 1) (Action ID: 2137)\n",
            "[[ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  1  0  0  0  1]\n",
            " [ 0  0 -1  0  0 -1  0  0]\n",
            " [-1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  1  0  0  0  1]\n",
            " [ 0  0 -1  0  0 -1  0  0]\n",
            " [-1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 5) -> (3, 5) (Action ID: 2397)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0 -1  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  0]]\n",
            "Reward: 2.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (5, 7) -> (7, 7) (Action ID: 3071)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0 -1  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 5) -> (3, 2) (Action ID: 1882)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 4) -> (2, 4) (Action ID: 788)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 2) -> (5, 2) (Action ID: 2218)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (2, 4) -> (1, 4) (Action ID: 1292)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0 -1  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (5, 2) -> (5, 4) (Action ID: 2732)\n",
            "[[ 0  0  0  0  1  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 4) -> (0, 3) (Action ID: 259)\n",
            "[[ 0  0  0  1  0  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [-1  0  0  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (5, 0) -> (5, 2) (Action ID: 2602)\n",
            "[[ 0  0  0  1  0  0  0  1]\n",
            " [ 1  0  0  0  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 3) -> (1, 3) (Action ID: 203)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1 -1  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 2) -> (2, 2) (Action ID: 1682)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 2) -> (4, 2) (Action ID: 1186)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 0) -> (0, 0) (Action ID: 512)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (3, 1) (Action ID: 1113)\n",
            "[[ 1  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 0) -> (1, 0) (Action ID: 8)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (3, 1) -> (2, 1) (Action ID: 1617)\n",
            "[[ 0  0  0  0  0  0  0  1]\n",
            " [ 1  0  0  1  1  0  0  0]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 7) -> (1, 7) (Action ID: 463)\n",
            "[[ 0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  1  1  0  0  1]\n",
            " [ 0 -1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (2, 1) -> (0, 1) (Action ID: 1089)\n",
            "[[ 0 -1  0  0  0  0  0  0]\n",
            " [ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 3) -> (0, 3) (Action ID: 707)\n",
            "[[ 0 -1  0  1  0  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (0, 1) -> (0, 0) (Action ID: 64)\n",
            "[[-1  0  0  1  0  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (0, 3) -> (1, 3) (Action ID: 203)\n",
            "[[-1  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0 -1  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (4, 2) -> (0, 2) (Action ID: 2178)\n",
            "[[-1  0 -1  0  0  0  0  0]\n",
            " [ 1  0  0  1  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player 1's turn\n",
            "Agent chooses move: (1, 3) -> (0, 3) (Action ID: 707)\n",
            "[[-1  0 -1  1  0  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0 -1  0  0  1]]\n",
            "Reward: 0.0, Done: False\n",
            "\n",
            "Player -1's turn\n",
            "Agent chooses move: (7, 4) -> (7, 5) (Action ID: 3901)\n",
            "[[-1  0 -1  1  0  0  0  0]\n",
            " [ 1  0  0  0  1  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1]\n",
            " [ 0  0  0  0  0  0  0  0]\n",
            " [ 0  0 -1  0 -1  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0  0]\n",
            " [ 0  0  0  0  0 -1  0  1]]\n",
            "Reward: 5.0, Done: True\n",
            "Game ended in a draw!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qSHmDXiJT6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}